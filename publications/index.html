<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Swabha  Swayamdipta | publications</title>
<meta name="description" content="">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">

<!-- Theming-->


    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Swabha</span> 
      </a>
      
      
        <!-- Social Icons -->
        <div class="row ml-1 ml-sm-0">
          <span class="contact-icon text-center">
  <a href="mailto:%73%77%61%62%68%61%73@%75%73%63.%65%64%75"><i class="fas fa-envelope"></i></a>
  
  <a href="https://scholar.google.com/citations?user=3uTVQt0AAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
  <a href="https://semanticscholar.com/author/2705113" target="_blank" title="Semantic Scholar"><i class="ai ai-semantic-scholar"></i></a>
  
  
  <a href="https://github.com/swabhs" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
  
  <a href="https://twitter.com/swabhz" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>
  
  
  
  
  
</span>

        </div>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/mentoring/">
                mentoring
                
              </a>
          </li>
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/prospective/">
                prospective
                
              </a>
          </li>
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/talks/">
                talks
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="desc">Also see <a href="https://scholar.google.com/citations?user=3uTVQt0AAAAJ">Google Scholar</a> and <a href="https://www.semanticscholar.org/author/Swabha-Swayamdipta/2705113">Semantic Scholar</a></p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2022</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="chen2022rev" class="col-sm-8">
    
      
        <div class="title"><a href="https://arxiv.org/abs/2210.04982" target="_blank"><b>REV: Information-Theoretic Evaluation of Free-Text Rationales</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Hanjie Chen,
                
              
            
          
        
          
            
              
                
                  Faeze Brahman,
                
              
            
          
        
          
            
              
                
                  Xiang Ren,
                
              
            
          
        
          
            
              
                
                  Yangfeng Ji,
                
              
            
          
        
          
            
              
                
                  Yejin Choi,
                
              
            
          
        
          
            
              
                and <span style="color:cadetblue">Swabha Swayamdipta</span>
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2022
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Free-text rationales are a promising step towards explainable AI, yet their evaluation remains an open research problem. While existing metrics have mostly focused on measuring the direct association between the rationale and a given label, we argue that an ideal metric should also be able to focus on the new information uniquely provided in the rationale that is otherwise not provided in the input or the label. We investigate this research problem from an information-theoretic perspective using the conditional V-information. More concretely, we propose a metric called REV (Rationale Evaluation with conditional V-information), that can quantify the new information in a rationale supporting a given label beyond the information already available in the input or the label. Experiments on reasoning tasks across four benchmarks, including few-shot prompting with GPT-3, demonstrate the effectiveness of REV in evaluating different types of rationale-label pairs, compared to existing metrics. Through several quantitative comparisons, we demonstrate the capability of REV in providing more sensitive measurements of new information in free-text rationales with respect to a label. Furthermore, REV is consistent with human judgments on rationale evaluations. Overall, when used alongside traditional performance metrics, REV provides deeper insights into a models‚Äô reasoning and prediction processes.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="sun2022investigating" class="col-sm-8">
    
      
        <div class="title"><a href="https://arxiv.org/abs/2206.11083" target="_blank"><b>Investigating the Benefits of Free-Form Rationales</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Jiao Sun,
                
              
            
          
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  Jonathan May,
                
              
            
          
        
          
            
              
                
                  and Xuezhe Ma
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Findings of EMNLP</em>
      
      
        2022
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Free-form rationales aim to aid model interpretability by supplying the background knowledge that can help understand model decisions. Crowdsourced rationales are provided for commonsense QA instances in popular datasets such as CoS-E and ECQA, but their utility remains under-investigated. We present human studies which show that ECQA rationales indeed provide additional background information to understand a decision, while over 88% of CoS-E rationales do not. Inspired by this finding, we ask: can the additional context provided by free-form rationales benefit models, similar to human users? We investigate the utility of rationales as an additional source of supervision, by varying the quantity and quality of rationales during training. After controlling for instances where rationales leak the correct answer while not providing additional background knowledge, we find that incorporating only 5% of rationales during training can boost model performance by 47.22% for CoS-E and 57.14% for ECQA during inference. Moreover, we also show that rationale quality matters: compared to crowdsourced rationales, T5-generated rationales provide not only weaker supervision to models, but are also not helpful for humans in aiding model interpretability.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="howard2022neurocf" class="col-sm-8">
    
      
        <div class="title"><a href="https://arxiv.org/abs/2210.12365" target="_blank"><b>NeuroCounterfactuals: Beyond Minimal-Edit Counterfactuals for Richer Data Augmentation</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Phillip Howard,
                
              
            
          
        
          
            
              
                
                  Gadi Singer,
                
              
            
          
        
          
            
              
                
                  Vasudev Lal,
                
              
            
          
        
          
            
              
                
                  Yejin Choi,
                
              
            
          
        
          
            
              
                and <span style="color:cadetblue">Swabha Swayamdipta</span>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Findings of EMNLP</em>
      
      
        2022
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>While counterfactual data augmentation offers a promising step towards robust generalization in natural language processing, producing a set of counterfactuals that offer valuable inductive bias for models remains a challenge. Most existing approaches for producing counterfactuals, manual or automated, rely on small perturbations via minimal edits, resulting in simplistic changes. We introduce NeuroCounterfactuals, designed as loose counterfactuals, allowing for larger edits which result in naturalistic generations containing linguistic diversity, while still bearing similarity to the original document. Our novel generative approach bridges the benefits of constrained decoding, with those of language model adaptation for sentiment steering. Training data augmentation with our generations results in both in-domain and out-of-domain improvements for sentiment classification, outperforming even manually curated counterfactuals, under select settings. We further present detailed analyses to show the advantages of NeuroCounterfactuals over approaches involving simple, minimal edits.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="liu2022wanli" class="col-sm-8">
    
      
        <div class="title"><a href="https://arxiv.org/abs/2201.05955" target="_blank"><b>WaNLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Alisa Liu,
                
              
            
          
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  Noah A. Smith,
                
              
            
          
        
          
            
              
                
                  and Yejin Choi
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Findings of EMNLP</em>
      
      
        2022
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
      <abbr class="badge repo"><a href="https://github.com/alisawuffles/wanli" target="_blank">Code</a></abbr>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>A recurring challenge of crowdsourcing NLP datasets at scale is that human writers often rely on repetitive patterns when crafting examples, leading to a lack of linguistic diversity. We introduce a novel paradigm for dataset creation based on human and machine collaboration, which brings together the generative strength of language models and the evaluative strength of humans. Starting with an existing dataset, MultiNLI, our approach uses dataset cartography to automatically identify examples that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose new examples with similar patterns. Machine generated examples are then automatically filtered, and finally revised and labeled by human crowdworkers to ensure quality. The resulting dataset, WANLI, consists of 108,357 natural language inference (NLI) examples that present unique empirical strengths over existing NLI datasets. Remarkably, training a model on WANLI instead of MNLI (which is 4 times larger) improves performance on seven out-of-domain test sets we consider, including by 11% on HANS and 9% on Adversarial NLI. Moreover, combining MNLI with WANLI is more effective than combining with other augmentation sets that have been introduced. Our results demonstrate the potential of natural language generation techniques to curate NLP datasets of enhanced quality and diversity.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NAACL</abbr>
    
  
  </div>

  <div id="deeplo-2022-deep" class="col-sm-8">
    
      
        <div class="title"><a href="https://aclanthology.org/2022.deeplo-1.0" target="_blank"><b>Proceedings of the Third Workshop on Deep Learning for Low-Resource Natural Language Processing</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Colin Cherry,
                
              
            
          
        
          
            
              
                
                  Angela Fan,
                
              
            
          
        
          
            
              
                
                  George Foster,
                
              
            
          
        
          
            
              
                
                  Gholamreza (Reza) Haffari,
                
              
            
          
        
          
            
              
                
                  Shahram Khadivi,
                
              
            
          
        
          
            
              
                
                  Nanyun (Violet) Peng,
                
              
            
          
        
          
            
              
                
                  Xiang Ren,
                
              
            
          
        
          
            
              
                
                  Ehsan Shareghi,
                
              
            
          
        
          
            
              
                and <span style="color:cadetblue">Swabha Swayamdipta</span>
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2022
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The NAACL 2022 Workshop on Deep Learning Approaches for Low-Resource Natural Language Processing (DeepLo) takes place on Thursday, July 22, in Seattle Washington, USA, immediately after the main conference. Natural Language Processing is being revolutionized by deep learning. However, deep learning requires large amounts of annotated data, and its advantage over traditional statistical methods typically diminishes when such data is not available. Large amounts of annotated data simply do not exist for many low-resource languages. Even for high-resource languages it can be difficult to find linguistically annotated data of sufficient size and quality to allow neural methods to excel; this remains true even as few-shot learning approaches have gained popularity in recent years. This workshop aims to bring together researchers from the NLP and ML communities who work on learning with neural methods when there is not enough data for those methods to succeed out-of-the-box. Specifically, it will provide attendees with an overview of new and existing approaches from various disciplines, and enable them to distill principles that can be more generally applicable. We will also discuss the main challenges arising in this setting, and outline potential directions for future progress. Our program covers a broad spectrum of applications and techniques. It is augmented by invited talks from Yulia Tsvetkov, Sebastian Ruder, Graham Neubig, and David Ifeoluwa Adelani. We would like to thank the members of our Program Committee for their timely and thoughtful reviews.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NAACL</abbr>
    
  
  </div>

  <div id="wiegreffe2021reframing" class="col-sm-8">
    
      
        <div class="title"><a href="https://arxiv.org/abs/2112.08674" target="_blank"><b>Reframing Human-AI Collaboration for Generating Free-Text Explanations</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Sarah Wiegreffe,
                
              
            
          
        
          
            
              
                
                  Jack Hessel,
                
              
            
          
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  Mark Riedl,
                
              
            
          
        
          
            
              
                
                  and Yejin Choi
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of NAACL</em>
      
      
        2022
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
      <abbr class="badge repo"><a href="https://github.com/allenai/few_shot_explanations/" target="_blank">Code</a></abbr>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Large language models are increasingly capable of generating fluent-appearing text with relatively little task-specific supervision. But can these models accurately explain classification decisions? We consider the task of generating free-text explanations using a small number of human-written examples (i.e., in a few-shot manner). We find that (1) authoring higher-quality examples for prompting results in higher quality generations; and (2) surprisingly, in a head-to-head comparison, crowdworkers often prefer explanations generated by GPT-3 to crowdsourced human-written explanations contained within existing datasets. Crowdworker ratings also show, however, that while models produce factual, grammatical, and sufficient explanations, they have room to improve, e.g., along axes such as providing novel information and supporting the label. We create a pipeline that combines GPT-3 with a supervised filter that incorporates humans-in-the-loop via binary acceptability judgments. Despite significant subjectivity intrinsic to judging acceptability, our approach is able to consistently filter GPT-3 generated explanations deemed acceptable by humans.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NAACL</abbr>
    
  
  </div>

  <div id="sap2021annotators" class="col-sm-8">
    
      
        <div class="title"><a href="https://arxiv.org/abs/2111.07997" target="_blank"><b>Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Maarten Sap,
                
              
            
          
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  Laura Vianna,
                
              
            
          
        
          
            
              
                
                  Xuhui Zhou,
                
              
            
          
        
          
            
              
                
                  Yejin Choi,
                
              
            
          
        
          
            
              
                
                  and Noah A. Smith
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of NAACL</em>
      
      
        2022
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The perceived toxicity of language can vary based on someone‚Äôs identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the who, why, and what behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (who) and beliefs (why), drawing from social psychology research about hate speech, free speech, racist beliefs, political leaning, and more. We disentangle what is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity. Our results show strong associations between annotator identity and beliefs and their ratings of toxicity. Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic. We additionally present a case study illustrating how a popular toxicity detection system‚Äôs ratings inherently reflect only specific beliefs and perspectives. Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  </div>

  <div id="ethayarajh2021informationtheoretic" class="col-sm-8">
    
      
        <div class="title"><a href="https://arxiv.org/abs/2110.08420" target="_blank"><b>Understanding Dataset Difficulty with ùí±-Usable Information</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Kawin Ethayarajh,
                
              
            
          
        
          
            
              
                
                  Yejin Choi,
                
              
            
          
        
          
            
              
                and <span style="color:cadetblue">Swabha Swayamdipta</span>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of ICML</em>
      
      
        2022
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
      <abbr class="badge repo"><a href="https://github.com/kawine/dataset_difficulty" target="_blank">Code</a></abbr>
    
    
    
    
    
    
      <abbr class="badge award">Outstanding Paper Award</abbr>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty ‚Äì w.r.t. a model ùí± ‚Äì as the lack of ùí±-usable information (Xu et al., 2019), where a lower value indicates a more difficult dataset for ùí±. We further introduce pointwise ÓâÇ-information (PVI) for measuring the difficulty of individual instances w.r.t. a given distribution. While standard evaluation metrics typically only compare different models for the same dataset, ùí±-usable information and PVI also permit the converse: for a given model ùí±, we can compare different datasets, as well as different instances/slices of the same dataset. Furthermore, our framework allows for the interpretability of different input attributes via transformations of the input, which we use to discover annotation artefacts in widely-used NLP benchmarks.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
    
  
  </div>

  <div id="pillutla2021mauve" class="col-sm-8">
    
      
        <div class="title"><a href="https://arxiv.org/abs/2102.01454" target="_blank"><b>MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Krishna Pillutla,
                
              
            
          
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  Rowan Zellers,
                
              
            
          
        
          
            
              
                
                  John Thickstun,
                
              
            
          
        
          
            
              
                
                  Sean Wellecks,
                
              
            
          
        
          
            
              
                
                  Yejin Choi,
                
              
            
          
        
          
            
              
                
                  and Zaid Harchaoui
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of NeurIPS</em>
      
      
        2021
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
      <abbr class="badge repo"><a href="https://github.com/krishnap25/mauve" target="_blank">Code</a></abbr>
    
    
    
    
    
    
      <abbr class="badge award">Outstanding Paper Award</abbr>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>As major progress is made in open-ended text generation, measuring how close machine-generated text is to human language remains a critical open problem. We introduce MAUVE, a comparison measure for open-ended text generation, which directly compares the learnt distribution from a text generation model to the distribution of human-written text using divergence frontiers. MAUVE scales up to modern text generation models by computing information divergences in a quantized embedding space. Through an extensive empirical study on three open-ended generation tasks, we find that MAUVE identifies known properties of generated text, scales naturally with model size, and correlates with human judgments, with fewer restrictions than existing distributional evaluation metrics.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="pancholy2021sister" class="col-sm-8">
    
      
        <div class="title"><a href="http://arxiv.org/abs/2109.07725" target="_blank"><b>Sister Help: Data Augmentation for Frame-Semantic Role Labeling</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Ayush Pancholy,
                
              
            
          
        
          
            
              
                
                  Miriam R. L. Petruck,
                
              
            
          
        
          
            
              
                and <span style="color:cadetblue">Swabha Swayamdipta</span>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of LAW-DMR Workshop at EMNLP</em>
      
      
        2021
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
      <abbr class="badge repo"><a href="https://github.com/ayush-pancholy/sister-help" target="_blank">Code</a></abbr>
    
    
    
      
      <abbr class="badge pdfs"><a href="/assets/pdf/posters/sisters-ayush-lawdmr-emnlp2021.pdf">Poster</a></abbr>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>While FrameNet is widely regarded as a rich resource of semantics in natural language processing, a major criticism concerns its lack of coverage and the relative paucity of its labeled data compared to other commonly used lexical resources such as PropBank and VerbNet. This paper reports on a pilot study to address these gaps. We propose a data augmentation approach, which uses existing frame-specific annotation to automatically annotate other lexical units of the same frame which are unannotated. Our rule-based approach defines the notion of a sister lexical unit and generates frame-specific augmented data for training. We present experiments on frame-semantic role labeling which demonstrate the importance of this data augmentation: we obtain a large improvement to prior results on frame identification and argument identification for FrameNet, utilizing both full-text and lexicographic annotations under FrameNet. Our findings on data augmentation highlight the value of automatic resource creation for improved models in frame-semantic parsing.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="jacovi2021contrastive" class="col-sm-8">
    
      
        <div class="title"><a href="https://arxiv.org/abs/2103.01378" target="_blank"><b>Contrastive Explanations for Model Interpretability</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Alon Jacovi,
                
              
            
          
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  Shauli Ravfogel,
                
              
            
          
        
          
            
              
                
                  Yanai Elazar,
                
              
            
          
        
          
            
              
                
                  Yejin Choi,
                
              
            
          
        
          
            
              
                
                  and Yoav Goldberg
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of EMNLP</em>
      
      
        2021
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
      <abbr class="badge repo"><a href="https://github.com/allenai/contrastive-explanations" target="_blank">Code</a></abbr>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Contrastive explanations clarify why an event occurred in contrast to another. They are more inherently intuitive to humans to both produce and comprehend. We propose a methodology to produce contrastive explanations for classification models by modifying the representation to disregard non-contrastive information, and modifying model behavior to only be based on contrastive reasoning. Our method is based on projecting model representation to a latent space that captures only the features that are useful (to the model) to differentiate two potential decisions. We demonstrate the value of contrastive explanations by analyzing two different scenarios, using both high-level abstract concept attribution and low-level input token/span attribution, on two widely used text classification tasks. Specifically, we produce explanations for answering: for which label, and against which alternative label, is some aspect of the input useful? And which aspects of the input are useful for and against particular decisions? Overall, our findings shed light on the ability of label-contrastive explanations to provide a more accurate and finer-grained interpretability of a model‚Äôs decision.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="liu2021onthefly" class="col-sm-8">
    
      
        <div class="title"><a href="https://arxiv.org/abs/2105.03023" target="_blank"><b>DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Alisa Liu,
                
              
            
          
        
          
            
              
                
                  Maarten Sap,
                
              
            
          
        
          
            
              
                
                  Ximing Lu,
                
              
            
          
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  Chandra Bhagavatula,
                
              
            
          
        
          
            
              
                
                  Noah A. Smith,
                
              
            
          
        
          
            
              
                
                  and Yejin Choi
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of ACL</em>
      
      
        2021
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
      <abbr class="badge repo"><a href="https://github.com/alisawuffles/DExperts" target="_blank">Code</a></abbr>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Despite recent advances in natural language generation, it remains challenging to control attributes of generated text. We propose DExperts: Decoding-time Experts, a decoding-time method for controlled text generation that combines a pretrained language model with "expert" LMs and/or "anti-expert" LMs in a product of experts. Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts. We apply DExperts to language detoxification and sentiment-controlled generation, where we outperform existing controllable generation methods on both automatic and human evaluations. Moreover, because DExperts operates only on the output of the pretrained LM, it is effective with (anti-)experts of smaller size, including when operating on GPT-3. Our work highlights the promise of tuning small LMs on text with (un)desirable attributes for efficient decoding-time steering.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EACL</abbr>
    
  
  </div>

  <div id="Zhou2021ToxicDebias" class="col-sm-8">
    
      
        <div class="title"><a href="https://arxiv.org/abs/2102.00086" target="_blank"><b>Challenges in Automated Debiasing for Toxic Language Detection</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Xuhui Zhou,
                
              
            
          
        
          
            
              
                
                  Maarten Sap,
                
              
            
          
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  Noah A. Smith,
                
              
            
          
        
          
            
              
                
                  and Yejin Choi
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of EACL</em>
      
      
        2021
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
      <abbr class="badge repo"><a href="https://github.com/XuhuiZhou/Toxic_Debias" target="_blank">Code</a></abbr>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Biased associations have been a challenge in the development of classifiers for detecting toxic language, hindering both fairness and accuracy. As potential solutions, we investigate recently introduced debiasing methods for text classification datasets and models, as applied to toxic language detection. Our focus is on lexical (e.g., swear words, slurs, identity mentions) and dialectal markers (specifically African American English). Our comprehensive experiments establish that existing methods are limited in their ability to prevent biased behavior in current toxicity detectors. We then propose an automatic, dialect-aware data correction method, as a proof-of-concept. Despite the use of synthetic labels, this method reduces dialectal associations with toxicity. Overall, our findings show that debiasing a model trained on biased toxic language data is not as effective as simply relabeling the data to remove existing biases.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="swayamdipta2020datamaps" class="col-sm-8">
    
      
        <div class="title"><a href="https://arxiv.org/abs/2009.10795" target="_blank"><b>Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics</b></a></div>
      
      <div class="author">
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  Roy Schwartz,
                
              
            
          
        
          
            
              
                
                  Nicholas Lourie,
                
              
            
          
        
          
            
              
                
                  Yizhong Wang,
                
              
            
          
        
          
            
              
                
                  Hannaneh Hajishirzi,
                
              
            
          
        
          
            
              
                
                  Noah A. Smith,
                
              
            
          
        
          
            
              
                
                  and Yejin Choi
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of EMNLP</em>
      
      
        2020
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
      <abbr class="badge repo"><a href="https://github.com/allenai/cartography" target="_blank">Code</a></abbr>
    
    
    
    
      
      <abbr class="badge pdfs"><a href="https://slideslive.com/38939175" target="_blank">Slides</a></abbr>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps‚Äîa model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example‚Äîthe model‚Äôs confidence in the true class, and the variability of this confidence across epochs‚Äîobtained in a single run of training. Experiments across four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of "ambiguous" regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are "easy to learn" for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds "hard to learn"; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="gururangan2020dont" class="col-sm-8">
    
      
        <div class="title"><a href="https://arxiv.org/abs/2004.10964" target="_blank"><b>Don‚Äôt Stop Pretraining: Adapt Language Models to Domains and Tasks</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Suchin Gururangan,
                
              
            
          
        
          
            
              
                
                  Ana Marasoviƒá,
                
              
            
          
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  Kyle Lo,
                
              
            
          
        
          
            
              
                
                  Iz Beltagy,
                
              
            
          
        
          
            
              
                
                  Doug Downey,
                
              
            
          
        
          
            
              
                
                  and Noah A. Smith
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of ACL</em>
      
      
        2020
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
      <abbr class="badge repo"><a href="https://github.com/allenai/dont-stop-pretraining" target="_blank">Code</a></abbr>
    
    
    
    
    
    
      <abbr class="badge award">Best Paper Honorable Mention</abbr>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Language models pretrained on text from a wide variety of sources form the foundation of today‚Äôs NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task‚Äôs unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  </div>

  <div id="bras2020adversarial" class="col-sm-8">
    
      
        <div class="title"><a href="https://arxiv.org/abs/2002.04108" target="_blank"><b>Adversarial Filters of Dataset Biases</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Ronan LeBras,
                
              
            
          
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  Chandra Bhagavatula,
                
              
            
          
        
          
            
              
                
                  Rowan Zellers,
                
              
            
          
        
          
            
              
                
                  Matthew E. Peters,
                
              
            
          
        
          
            
              
                
                  Ashish Sabharwal,
                
              
            
          
        
          
            
              
                
                  and Yejin Choi
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of ICML</em>
      
      
        2020
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
      <abbr class="badge repo"><a href="https://github.com/swabhs/notebooks_for_aflite" target="_blank">Code</a></abbr>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Large neural models have demonstrated human-level performance on language and vision benchmarks, while their performance degrades considerably on adversarial or out-of-distribution samples. This raises the question of whether these models have learned to solve a dataset rather than the underlying task by overfitting to spurious dataset biases. We investigate one recently proposed approach, AFLite, which adversarially filters such dataset biases, as a means to mitigate the prevalent overestimation of machine performance. We provide a theoretical understanding for AFLite, by situating it in the generalized framework for optimum bias reduction. We present extensive supporting evidence that AFLite is broadly applicable for reduction of measurable dataset biases, and that models trained on the filtered datasets yield better generalization to out-of-distribution tasks. Finally, filtering results in a large drop in model performance (e.g., from 92% to 62% for SNLI), while human performance still remains high. Our work thus shows that such filtered datasets can pose new research challenges for robust generalization by serving as upgraded benchmarks. </p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="schwartz2020right" class="col-sm-8">
    
      
        <div class="title"><a href="https://arxiv.org/abs/2004.07453" target="_blank"><b>The Right Tool for the Job: Matching Model and Instance Complexities</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Roy Schwartz,
                
              
            
          
        
          
            
              
                
                  Gabi Stanovsky,
                
              
            
          
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  Jesse Dodge,
                
              
            
          
        
          
            
              
                
                  and Noah A. Smith
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of ACL</em>
      
      
        2020
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
      <abbr class="badge repo"><a href="https://github.com/allenai/sledgehammer" target="_blank">Code</a></abbr>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>As NLP models become larger, executing a trained model requires significant computational resources incurring monetary and environmental costs. To better respect a given inference budget, we propose a modification to contextual representation fine-tuning which, during inference, allows for an early (and fast) "exit" from neural network calculations for simple instances, and late (and accurate) exit for hard instances. To achieve this, we add classifiers to different layers of BERT and use their calibrated confidence scores to make early exit decisions. We test our proposed modification on five different datasets in two tasks: three text classification datasets and two natural language inference benchmarks. Our method presents a favorable speed/accuracy tradeoff in almost all cases, producing models which are up to five times faster than the state of the art, while preserving their accuracy. Our method also requires almost no additional training resources (in either time or parameters) compared to the baseline BERT model. Finally, our method alleviates the need for costly retraining of multiple models at different levels of efficiency; we allow users to control the inference speed/accuracy tradeoff using a single trained model, by setting a single variable at inference time. We publicly release our code.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="yang2020gdaug" class="col-sm-8">
    
      
        <div class="title"><a href="https://arxiv.org/abs/2004.11546" target="_blank"><b>G-DAUG: Generative Data Augmentation for Commonsense Reasoning</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Yiben Yang,
                
              
            
          
        
          
            
              
                
                  Chaitanya Malaviya,
                
              
            
          
        
          
            
              
                
                  Jared Fernandez,
                
              
            
          
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  Ronan LeBras,
                
              
            
          
        
          
            
              
                
                  Ji-Ping Wang,
                
              
            
          
        
          
            
              
                
                  Chandra Bhagavatula,
                
              
            
          
        
          
            
              
                
                  Yejin Choi,
                
              
            
          
        
          
            
              
                
                  and Doug Downey
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of EMNLP</em>
      
      
        2020
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
      <abbr class="badge repo"><a href="https://github.com/yangyiben/G-DAUG-c-Generative-Data-Augmentation-for-Commonsense-Reasoning" target="_blank">Code</a></abbr>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Recent advances in commonsense reasoning depend on large-scale human-annotated training data to achieve peak performance. However, manual curation of training examples is expensive and has been shown to introduce annotation artifacts that neural models can readily exploit and overfit on. We investigate G-DAUG^C, a novel generative data augmentation method that aims to achieve more accurate and robust learning in the low-resource setting. Our approach generates synthetic examples using pretrained language models, and selects the most informative and diverse set of examples for data augmentation. In experiments with multiple commonsense reasoning benchmarks, G-DAUG^C consistently outperforms existing data augmentation methods based on back-translation, and establishes a new state-of-the-art on WinoGrande, CODAH, and CommonsenseQA. Further, in addition to improvements in in-distribution accuracy, G-DAUG^C-augmented training also enhances out-of-distribution generalization, showing greater robustness against adversarial or perturbed examples. Our analysis demonstrates that G-DAUG^C produces a diverse set of fluent training examples, and that its selection and training approaches are important for performance. Our findings encourage future research toward generative data augmentation to enhance both in-distribution learning and out-of-distribution generalization.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">PhD</abbr>
    
  
  </div>

  <div id="swayamdipta2019syntactic" class="col-sm-8">
    
      
        <div class="title"><a href="https://swabhs.com/assets/pdf/swabha_thesis.pdf" target="_blank"><b>PhD Thesis: Syntactic Inductive Biases for Natural Language Processing</b></a></div>
      
      <div class="author">
        
          
            
              <span class="font-weight-bold">Swabha Swayamdipta</span>
            
          
        
      </div>

      <div class="periodical">
      
      
        2019
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>With the rise in availability of data for language learning, the role of linguistic structure is under scrutiny. The underlying syntactic structure of language allows for composition of simple elements into more complex ones in innumerable ways; generalization to new examples hinges on this structure. We define a syntactic inductive bias as a signal that steers the learning algorithm towards a syntactically robust solution, over others. This thesis explores the need for incorporation of such biases into already powerful neural models of language. We describe three general approaches for incorporating syntactic inductive biases into task-specific models, under different levels of supervision. The first method calls for joint learning of entire syntactic dependency trees with semantic dependency graphs through direct supervision, to facilitate better semantic dependency parsing. Second, we introduce the paradigm of scaffolded learning, which enables us to leverage inductive biases from syntactic sources to predict a related semantic structure, using only as much supervision as is necessary. The third approach yields general-purpose contextualized representations conditioned on large amounts of data along with their shallow syntactic structures, obtained automatically. The linguistic representations learned as a result of syntactic inductive biases are shown to be effective across a range of downstream tasks, but their usefulness is especially pronounced for semantic tasks.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NAACL</abbr>
    
  
  </div>

  <div id="Ruder:19" class="col-sm-8">
    
      
        <div class="title"><a href="https://www.aclweb.org/anthology/N19-5004/" target="_blank"><b>Tutorial on Transfer Learning in Natural Language Processing</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Sebastian Ruder,
                
              
            
          
        
          
            
              
                
                  Matthew E Peters,
                
              
            
          
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  and Thomas Wolf
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of NAACL</em>
      
      
        2019
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by leveraging data from additional domains or tasks to train a model with better generalization properties. Over the last two years, the field of Natural Language Processing (NLP) has witnessed the emergence of several transfer learning methods and architectures which significantly improved upon the state-of-the-art on a wide range of NLP tasks. These improvements together with the wide availability and ease of integration of these methods are reminiscent of the factors that led to the success of pretrained word embeddings and ImageNet pretraining in computer vision, and indicate that these methods will likely become a common tool in the NLP landscape as well as an important research direction. We will present an overview of modern transfer learning methods in NLP, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream NLP tasks.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="Swayamdipta:19" class="col-sm-8">
    
      
        <div class="title"><a href="https://arxiv.org/abs/1908.11047" target="_blank"><b>Shallow Syntax in Deep Water</b></a></div>
      
      <div class="author">
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  Matthew Peters,
                
              
            
          
        
          
            
              
                
                  Brendan Roof,
                
              
            
          
        
          
            
              
                
                  Chris Dyer,
                
              
            
          
        
          
            
              
                
                  and Noah A. Smith
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2019
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Shallow syntax provides an approximation of phrase-syntactic structure of sentences; it can be produced with high accuracy, and is computationally cheap to obtain. We investigate the role of shallow syntax-aware representations for NLP tasks using two techniques. First, we enhance the ELMo architecture to allow pretraining on predicted shallow syntactic parses, instead of just raw text, so that contextual embeddings make use of shallow syntactic context. Our second method involves shallow syntactic features obtained automatically on downstream task data. Neither approach leads to a significant gain on any of the four downstream tasks we considered relative to ELMo-only baselines. Further analysis using black-box probes confirms that our shallow-syntax-aware contextual embeddings do not transfer to linguistic tasks any more easily than ELMo‚Äôs embeddings. We take these findings as evidence that ELMo-style pretraining discovers representations which make additional awareness of shallow syntax redundant.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NAACL</abbr>
    
  
  </div>

  <div id="Gururangan:18" class="col-sm-8">
    
      
        <div class="title"><a href="https://arxiv.org/abs/1803.02324" target="_blank"><b>Annotation Artifacts in Natural Language Inference Data</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Suchin Gururangan,
                
              
            
          
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  Omer Levy,
                
              
            
          
        
          
            
              
                
                  Roy Schwartz,
                
              
            
          
        
          
            
              
                
                  Samuel Bowman,
                
              
            
          
        
          
            
              
                
                  and Noah A. Smith
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of NAACL</em>
      
      
        2018
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
      <abbr class="badge repo"><a href="https://github.com/swabhs/notebooks/blob/master/annotation_artifacts.ipynb" target="_blank">Code</a></abbr>
    
    
    
      
      <abbr class="badge pdfs"><a href="/assets/pdf/posters/artifacts-naacl.pdf">Poster</a></abbr>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to. We show that, in a significant portion of such data, this protocol leaves clues that make it possible to identify the label by looking only at the hypothesis, without observing the premise. Specifically, we show that a simple text categorization model can correctly classify the hypothesis alone in about 67% of SNLI (Bowman et. al, 2015) and 53% of MultiNLI (Williams et. al, 2017). Our analysis reveals that specific linguistic phenomena such as negation and vagueness are highly correlated with certain inference classes. Our findings suggest that the success of natural language inference models to date has been overestimated, and that the task remains a hard open problem.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="Swayamdipta:18b" class="col-sm-8">
    
      
        <div class="title"><a href="https://arxiv.org/abs/1808.10485" target="_blank"><b>Syntactic Scaffolds for Semantic Structures</b></a></div>
      
      <div class="author">
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  Sam Thomson,
                
              
            
          
        
          
            
              
                
                  Kenton Lee,
                
              
            
          
        
          
            
              
                
                  Luke Zettlemoyer,
                
              
            
          
        
          
            
              
                
                  Chris Dyer,
                
              
            
          
        
          
            
              
                
                  and Noah A. Smith
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In EMNLP</em>
      
      
        2018
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
      <abbr class="badge repo"><a href="https://github.com/swabhs/scaffolding" target="_blank">Code</a></abbr>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We introduce the syntactic scaffold, an approach to incorporating syntactic information into semantic tasks. Syntactic scaffolds avoid expensive syntactic processing at runtime, only making use of a treebank during training, through a multitask objective. We improve over strong baselines on PropBank semantics, frame semantics, and coreference resolution, achieving competitive performance on all three tasks.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
    
  
  </div>

  <div id="Swayamdipta:18a" class="col-sm-8">
    
      
        <div class="title"><a href="https://openreview.net/forum?id=HyRnez-RW" target="_blank"><b>Multi-Mention Learning for Reading Comprehension with Neural Cascades</b></a></div>
      
      <div class="author">
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  Ankur P Parikh,
                
              
            
          
        
          
            
              
                
                  and Tom Kwiatkowski
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of ICLR</em>
      
      
        2018
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
    
    
      
      <abbr class="badge pdfs"><a href="/assets/pdf/posters/triviaqa-iclr.pdf">Poster</a></abbr>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Reading comprehension is a challenging task, especially when executed across longer or across multiple evidence documents, where the answer is likely to reoccur. Existing neural architectures typically do not scale to the entire evidence, and hence, resort to selecting a single passage in the document (either via truncation or other means), and carefully searching for the answer within that passage. However, in some cases, this strategy can be suboptimal,  since by focusing on a specific passage, it becomes difficult to leverage multiple mentions of the same answer throughout the document. In this work, we take a different approach by constructing lightweight models that are combined in a cascade to find the answer. Each submodel consists only of feed-forward networks equipped with an attention mechanism, making it trivially parallelizable. We show that our approach can scale to approximately an order of magnitude larger evidence documents and can aggregate information from multiple mentions of each answer candidate across the document. Empirically, our approach achieves state-of-the-art performance on both the Wikipedia and web domains of the TriviaQA dataset, outperforming more complex, recurrent architectures.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NAACL</abbr>
    
  
  </div>

  <div id="Peng:18a" class="col-sm-8">
    
      
        <div class="title"><a href="http://aclweb.org/anthology/N18-1135" target="_blank"><b>Learning Joint Semantic Parsers from Disjoint Data</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Hao Peng,
                
              
            
          
        
          
            
              
                
                  Sam Thomson,
                
              
            
          
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  and Noah A. Smith
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of NAACL</em>
      
      
        2018
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
      <abbr class="badge repo"><a href="https://github.com/Noahs-ARK/NeurboParser" target="_blank">Code</a></abbr>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We present a new approach to learning a semantic parser from multiple datasets, even when the target semantic formalisms are drastically different and the underlying corpora do not overlap. We handle such ‚Äúdisjoint‚Äù data by treating annotations for unobserved formalisms as latent structured variables. Building on state-of-the-art baselines, we show improvements both in frame-semantic parsing and semantic dependency parsing by modeling them jointly.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="Mulcaire:18" class="col-sm-8">
    
      
        <div class="title"><a href="https://aclanthology.org/P18-2106/" target="_blank"><b>Polyglot Semantic Role Labeling</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Phoebe Mulcaire,
                
              
            
          
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  and Noah A. Smith
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of ACL</em>
      
      
        2018
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Previous approaches to multilingual semantic dependency parsing treat languages independently, without exploiting the similarities between semantic structures across languages. We experiment with a new approach where we combine resources from different languages in the CoNLL 2009 shared task to build a single polyglot semantic dependency parser. Notwithstanding the absence of parallel data, and the dissimilarity in annotations between languages, our approach results in improvement in parsing performance on several languages over a monolingual baseline. Analysis of the polyglot models‚Äô performance provides a new understanding of the similarities and differences between languages in the shared task.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">COLING</abbr>
    
  
  </div>

  <div id="baker-etal-2018-frame" class="col-sm-8">
    
      
        <div class="title"><a href="https://www.aclweb.org/anthology/C18-3003" target="_blank"><b>Frame Semantics across Languages: Towards a Multilingual FrameNet</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Collin F. Baker,
                
              
            
          
        
          
            
              
                
                  Michael Ellsworth,
                
              
            
          
        
          
            
              
                
                  Miriam R. L. Petruck,
                
              
            
          
        
          
            
              
                and <span style="color:cadetblue">Swabha Swayamdipta</span>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In COLING: Tutorial Abstracts</em>
      
      
        2018
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>FrameNet is a lexical resource that provides rich semantic representations of the core English vocabulary based on Fillmore‚Äôs Frame Semantics, with more than 200k manually annotated examples. Resources based on FrameNet have now been created for roughly a dozen languages. This workshop will present current research on aligning Frame Semantic resources across languages and automatic frame semantic parsing in English and other languages. We will explore the extent to which semantic frames are similar across languages and the implications for theories of semantic universals, the practice of translation (whether human or machine), and multilingual knowledge representation. Does not require prior familiarity with Frame Semantics.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2017</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="Neubig:17" class="col-sm-8">
    
      
        <div class="title"><a href="https://arxiv.org/abs/1701.03980" target="_blank"><b>DyNet: The Dynamic Neural Network Toolkit</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Graham Neubig,
                
              
            
          
        
          
            
              
                
                  Chris Dyer,
                
              
            
          
        
          
            
              
                
                  Yoav Goldberg,
                
              
            
          
        
          
            
              
                
                  Austin Matthews,
                
              
            
          
        
          
            
              
                
                  Waleed Ammar,
                
              
            
          
        
          
            
              
                
                  Antonios Anastasopoulos,
                
              
            
          
        
          
            
              
                
                  Miguel Ballesteros,
                
              
            
          
        
          
            
              
                
                  David Chiang,
                
              
            
          
        
          
            
              
                
                  Daniel Clothiaux,
                
              
            
          
        
          
            
              
                
                  Trevor Cohn,
                
              
            
          
        
          
            
              
                
                  Kevin Duh,
                
              
            
          
        
          
            
              
                
                  Manaal Faruqui,
                
              
            
          
        
          
            
              
                
                  Cynthia Gan,
                
              
            
          
        
          
            
              
                
                  Dan Garrette,
                
              
            
          
        
          
            
              
                
                  Yangfeng Ji,
                
              
            
          
        
          
            
              
                
                  Lingpeng Kong,
                
              
            
          
        
          
            
              
                
                  Adhiguna Kuncoro,
                
              
            
          
        
          
            
              
                
                  Gaurav Kumar,
                
              
            
          
        
          
            
              
                
                  Chaitanya Malaviya,
                
              
            
          
        
          
            
              
                
                  Paul Michel,
                
              
            
          
        
          
            
              
                
                  Yusuke Oda,
                
              
            
          
        
          
            
              
                
                  Matthew Richardson,
                
              
            
          
        
          
            
              
                
                  Naomi Saphra,
                
              
            
          
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  and Pengcheng Yin
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2017
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
      <abbr class="badge repo"><a href="https://github.com/clab/dynet" target="_blank">Code</a></abbr>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We describe DyNet, a toolkit for implementing neural network models based on dynamic declaration of network structure. In the static declaration strategy that is used in toolkits like Theano, CNTK, and TensorFlow, the user first defines a computation graph (a symbolic representation of the computation), and then examples are fed into an engine that executes this computation and computes its derivatives. In DyNet‚Äôs dynamic declaration strategy, computation graph construction is mostly transparent, being implicitly constructed by executing procedural code that computes the network outputs, and the user is free to use different network structures for each input. Dynamic declaration thus facilitates the implementation of more complicated network architectures, and DyNet is specifically designed to allow users to implement their models in a way that is idiomatic in their preferred programming language (C++ or Python). One challenge with dynamic declaration is that because the symbolic computation graph is defined anew for every training example, its construction must have low overhead. To achieve this, DyNet has an optimized C++ backend and lightweight graph representation. Experiments show that DyNet‚Äôs speeds are faster than or comparable with static declaration toolkits, and significantly faster than Chainer, another dynamic declaration toolkit. DyNet is released open-source under the Apache 2.0 license.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="Swayamdipta:17" class="col-sm-8">
    
      
        <div class="title"><a href="https://arxiv.org/abs/1706.09528" target="_blank"><b>Frame-Semantic Parsing with Softmax-Margin Segmental RNNs and a Syntactic Scaffold</b></a></div>
      
      <div class="author">
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  Sam Thomson,
                
              
            
          
        
          
            
              
                
                  Chris Dyer,
                
              
            
          
        
          
            
              
                
                  and Noah A. Smith
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
        2017
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
      <abbr class="badge repo"><a href="https://github.com/swabhs/open-sesame" target="_blank">Code</a></abbr>
    
    
    
      
      <abbr class="badge pdfs"><a href="/assets/pdf/posters/open-sesame.pdf">Poster</a></abbr>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We present a new, efficient frame-semantic parser that labels semantic arguments to FrameNet predicates. Built using an extension to the segmental RNN that emphasizes recall, our basic system achieves competitive performance without any calls to a syntactic parser. We then introduce a method that uses phrase-syntactic annotations from the Penn Treebank during training only, through a multitask objective; no parsing is required at training or test time. This "syntactic scaffold" offers a cheaper alternative to traditional syntactic pipelining, and achieves state-of-the-art performance.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2016</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CoNLL</abbr>
    
  
  </div>

  <div id="Swayamdipta:16" class="col-sm-8">
    
      
        <div class="title"><a href="https://www.aclweb.org/anthology/K16-1019" target="_blank"><b>Greedy, Joint Syntactic-Semantic Parsing with Stack LSTMs</b></a></div>
      
      <div class="author">
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  Miguel Ballesteros,
                
              
            
          
        
          
            
              
                
                  Chris Dyer,
                
              
            
          
        
          
            
              
                
                  and Noah A. Smith
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of CoNLL</em>
      
      
        2016
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
      <abbr class="badge repo"><a href="https://github.com/swabhs/joint-lstm-parser" target="_blank">Code</a></abbr>
    
    
    
    
      
      <abbr class="badge pdfs"><a href="/assets/pdf/talks/conll16.pdf">Slides</a></abbr>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We present a transition-based parser that jointly produces syntactic and semantic dependencies. It learns a representation of the entire algorithm state, using stack long short-term memories. Our greedy inference algorithm has linear time, including feature extraction. On the CoNLL 2008‚Äì9 English shared tasks, we obtain the best published parsing performance among models that jointly learn syntax and semantics.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2014</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="Kong:2014" class="col-sm-8">
    
      
        <div class="title"><a href="https://www.aclweb.org/anthology/D14-1108" target="_blank"><b>A Dependency Parser for Tweets</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Lingpeng Kong,
                
              
            
          
        
          
            
              
                
                  Nathan Schneider,
                
              
            
          
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  Bhatia Archna,
                
              
            
          
        
          
            
              
                
                  Chris Dyer,
                
              
            
          
        
          
            
              
                
                  and Noah A. Smith
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of EMNLP</em>
      
      
        2014
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
      <abbr class="badge repo"><a href="https://github.com/ikekonglp/TweeboParser" target="_blank">Code</a></abbr>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We describe a new dependency parser for English tweets, TWEEBOPARSER. The parser builds on several contributions: new syntactic annotations for a corpus of tweets (TWEEBANK), with conventions informed by the domain; adaptations to a statistical parsing algorithm; and a new approach to exploiting out-of-domain Penn Treebank data. Our experiments show that the parser achieves over 80% unlabeled attachment accuracy on our new, high-quality test set and measure the benefit of our contributions.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">SemEval</abbr>
    
  
  </div>

  <div id="thomson-etal-2014-cmu" class="col-sm-8">
    
      
        <div class="title"><a href="https://www.aclweb.org/anthology/S14-2027" target="_blank"><b>CMU: Arc-Factored, Discriminative Semantic Dependency Parsing</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Sam Thomson,
                
              
            
          
        
          
            
              
                
                  Brendan O‚ÄôConnor,
                
              
            
          
        
          
            
              
                
                  Jeffrey Flanigan,
                
              
            
          
        
          
            
              
                
                  David Bamman,
                
              
            
          
        
          
            
              
                
                  Jesse Dodge,
                
              
            
          
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  Nathan Schneider,
                
              
            
          
        
          
            
              
                
                  Chris Dyer,
                
              
            
          
        
          
            
              
                
                  and Noah A. Smith
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of SemEval</em>
      
      
        2014
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We present an arc-factored statistical model for semantic dependency parsing, as defined by the SemEval 2014 Shared Task 8 on Broad-Coverage Semantic Dependency Parsing. Our entry in the open track placed second in the competition.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">WMT</abbr>
    
  
  </div>

  <div id="matthews2014cmu" class="col-sm-8">
    
      
        <div class="title"><a href="https://www.aclweb.org/anthology/W14-3315" target="_blank"><b>The CMU machine translation systems at WMT 2014</b></a></div>
      
      <div class="author">
        
          
            
              
                
                  Austin Matthews,
                
              
            
          
        
          
            
              
                
                  Waleed Ammar,
                
              
            
          
        
          
            
              
                
                  Archna Bhatia,
                
              
            
          
        
          
            
              
                
                  Weston Feely,
                
              
            
          
        
          
            
              
                
                  Greg Hanneman,
                
              
            
          
        
          
            
              
                
                  Eva Schlinger,
                
              
            
          
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  Yulia Tsvetkov,
                
              
            
          
        
          
            
              
                
                  Alon Lavie,
                
              
            
          
        
          
            
              
                
                  and Chris Dyer
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of WMT</em>
      
      
        2014
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We describe the CMU systems submitted to the 2014 WMT shared translation task. We participated in two language pairs, German‚ÄìEnglish and Hindi‚ÄìEnglish. Our innovations include: a label coarsening scheme for syntactic tree-to-tree translation, a host of new discriminative features, several modules to create ‚Äúsynthetic translation options‚Äù that can generalize beyond what is directly observed in the training data, and a method of combining the output of multiple word aligners to uncover extra phrase pairs and grammar rules.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2012</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICSC</abbr>
    
  
  </div>

  <div id="swayamdipta2012pursuit" class="col-sm-8">
    
      
        <div class="title"><a href="https://ieeexplore.ieee.org/abstract/document/6337078" target="_blank"><b>The Pursuit of Power and its Manifestation in Written Dialog</b></a></div>
      
      <div class="author">
        
          
            
              
                <span style="color:cadetblue">Swabha Swayamdipta</span>,
              
            
          
        
          
            
              
                
                  and Owen Rambow
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of ICSC</em>
      
      
        2012
      
      </div>
    

    <div class="links abbr">
    
      <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this paper we explore the written dialog behavior of participants in anon line discussion for automatic identification of participants who pursue power within the discussion group. We employ various standard unsupervised machine learning approaches to make this prediction. Our approach relies on the identification of certain discourse structures and linguistic techniques used by participants in the discussion. We achive an F-measure of 69.5% using unsupervised methods.</p>
    </div>
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2022 Swabha  Swayamdipta.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.

    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  



<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
