<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Sumanth Doddapaneni | publications</title>
  <meta name="description" content="">

  <!-- Open Graph -->


  <!-- Bootstrap & MDB -->
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg=="
    crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css"
    integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q=="
    crossorigin="anonymous" />

  <!-- Fonts & Icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"
    integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog=="
    crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css"
    integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg=="
    crossorigin="anonymous">
  <link rel="stylesheet" type="text/css"
    href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

  <!-- Code Syntax Highlighting -->
  <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

  <!-- Styles -->
  <link rel="shortcut icon" href="/assets/img/favicon.ico">
  <link rel="stylesheet" href="/assets/css/main.css">

  <link rel="canonical" href="/publications/">

  <!-- Theming-->



  <!-- MathJax -->
  <script defer type="text/javascript" id="MathJax-script"
    src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


</head>

<body class="fixed-top-nav ">

  <!-- Header -->

  <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
      <div class="container">

        <a class="navbar-brand title font-weight-lighter" href="/">
          <span class="font-weight-bold">Sumanth</span>
        </a>


        <!-- Social Icons -->
        <div class="row ml-1 ml-sm-0">
          <span class="contact-icon text-center">
            <a href="mailto:doddapaneni.sumanth@gmail.com"><i class="fas fa-envelope"></i></a>

            <a href="https://scholar.google.com/citations?user=_kR4rGAAAAAJ" target="_blank" title="Google Scholar"><i
                class="ai ai-google-scholar"></i></a>
            <a href="https://semanticscholar.com/author/2072738714" target="_blank" title="Semantic Scholar"><i
                class="ai ai-semantic-scholar"></i></a>


            <a href="https://github.com/sumanthd17" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>

            <a href="https://twitter.com/sumanthd17" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>





          </span>

        </div>

        <!-- Navbar Toogle -->
        <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav"
          aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar top-bar"></span>
          <span class="icon-bar middle-bar"></span>
          <span class="icon-bar bottom-bar"></span>
        </button>
        <div class="collapse navbar-collapse text-right" id="navbarNav">
          <ul class="navbar-nav ml-auto flex-nowrap">
            <!-- About -->
            <li class="nav-item active">
              <a class="nav-link" href="/">
                about

                <span class="sr-only">(current)</span>

              </a>
            </li>

            <!-- Other pages -->

            <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications

              </a>
            </li>

            <li class="nav-item ">
              <a class="nav-link" href="/assets/pdf/sumanthResume.pdf">
                resume
              </a>
            </li>

            <!-- <li class="nav-item ">
              <a class="nav-link" href="/mentoring/">
                mentoring
                
              </a>
          </li>

          <li class="nav-item ">
              <a class="nav-link" href="/prospective/">
                prospective
                
              </a>
          </li>

          <li class="nav-item ">
              <a class="nav-link" href="/talks/">
                talks
                
              </a>
          </li>

          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li> -->



          </ul>
        </div>
      </div>
    </nav>

  </header>


  <!-- Content -->

  <div class="container mt-5">
    <div class="post">

      <header class="post-header">
        <h1 class="post-title">publications</h1>
        <p class="desc">Also see <a href="https://scholar.google.com/citations?user=_kR4rGAAAAAJ">Google Scholar</a> and
          <a href="https://semanticscholar.com/author/2072738714">Semantic Scholar</a>
        </p>
      </header>

      <article>
        <div class="publications">

          <h2 class="year">2024</h2>
          <ol class="bibliography">

            <li>
              <div class="row">

                <div class="col-sm-2 abbr">
                  <abbr class="badge">ArXiv</abbr>
                </div>

                <div id="doddapaneni2024finding" class="col-sm-8">
                  <div class="title"><b>Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs</b>
                  </div>
                  <div class="author">
                    <font size="-1">
                      <span style="color:cadetblue">Sumanth
                        Doddapaneni</span>, Mohammed Safi Ur Rahman Khan, Dilip Venkatesh, Raj Dabre, Anoop
                      Kunchukuttan, Mitesh M. Khapra
                    </font>
                  </div>
                  <div class="links abbr">
                    <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a
                      href="https://arxiv.org/abs/2410.13394" target="_blank"><i class="fas fa-file"></i></a> <a
                      href="https://github.com/AI4Bharat/CIA" target="_blank"><i class="fab fa-github"></i></a>
                  </div>

                  <div class="abstract hidden">
                    <p>Evaluating machine-generated text remains a significant challenge in NLP, especially for
                      non-English languages. Current methodologies, including automated metrics, human assessments, and
                      LLM-based evaluations, predominantly focus on English, revealing a significant gap in multilingual
                      evaluation frameworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an extensible
                      framework that includes evaluator LLMs (Hercule) and a novel test set (Recon) specifically
                      designed for multilingual evaluation. Our test set features 500 human-annotated instructions
                      spanning various task capabilities along with human judgment scores across six languages. This
                      would enable benchmarking of general-purpose multilingual LLMs and facilitate meta-evaluation of
                      Evaluator LLMs. The proposed model, Hercule, is a cross-lingual evaluation model that addresses
                      the scarcity of reference answers in the target language by learning to assign scores to responses
                      based on easily available reference answers in English. Our experiments demonstrate that Hercule
                      aligns more closely with human judgments compared to proprietary models, demonstrating the
                      effectiveness of such cross-lingual evaluation in low resource scenarios. Further, it is also
                      effective in zero-shot evaluation on unseen languages. This study is the first comprehensive
                      examination of cross-lingual evaluation using LLMs, presenting a scalable and effective approach
                      for multilingual assessment. All code, datasets, and models will be publicly available to enable
                      further research in this important area.
                    </p>
                  </div>
                </div>
            </li>

            <li>
              <div class="row">

                <div class="col-sm-2 abbr">
                  <abbr class="badge">EMNLP'24</abbr>
                </div>

                <div id="doddapaneni2024finding" class="col-sm-8">
                  <div class="title"><b>Finding Blindspots in LLM Evaluations with Interpretable Checklists</b>
                  </div>
                  <div class="author">
                    <font size="-1">
                      <span style="color:cadetblue">Sumanth
                        Doddapaneni</span>, Mohammed Safi Ur Rahman Khan, Sshubam Verma, Mitesh M. Khapra
                    </font>
                  </div>
                  <div class="links abbr">
                    <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a
                      href="https://arxiv.org/abs/2406.13439" target="_blank"><i class="fas fa-file"></i></a> <a
                      href="https://github.com/AI4Bharat/FBI" target="_blank"><i class="fab fa-github"></i></a>
                  </div>

                  <div class="abstract hidden">
                    <p>Large Language Models (LLMs) are increasingly relied upon to evaluate text outputs of other
                      LLMs, thereby influencing leaderboards and development decisions. However, concerns persist
                      over the accuracy of these assessments and the potential for misleading conclusions. In this
                      work, we investigate the effectiveness of LLMs as evaluators for text generation tasks. We
                      propose FBI, a novel framework designed to examine the proficiency of Evaluator LLMs in
                      assessing four critical abilities in other LLMs: factual accuracy, instruction following,
                      coherence in long-form writing, and reasoning proficiency. By introducing targeted
                      perturbations in answers generated by LLMs, that clearly impact one of these key capabilities,
                      we test whether an Evaluator LLM can detect these quality drops. By creating a total of 2400
                      perturbed answers covering 22 perturbation categories, we conduct a comprehensive study using
                      different evaluation strategies on five prominent LLMs commonly used as evaluators in the
                      literature. Our findings reveal significant shortcomings in current Evaluator LLMs, which
                      failed to identify quality drops in over 50\% of cases on average. Single-answer and pairwise
                      evaluations demonstrated notable limitations, whereas reference-based evaluations showed
                      comparatively better performance. These results underscore the unreliable nature of current
                      Evaluator LLMs and advocate for cautious implementation in practical applications. Code and
                      data are available at this https://github.com/AI4Bharat/FBI.
                    </p>
                  </div>
                </div>
            </li>


            <li>
              <div class="row">

                <div class="col-sm-2 abbr">
                  <abbr class="badge">ACL</abbr>
                </div>

                <div id="ai4bharat2023indictrans2" class="col-sm-8">
                  <div class="title"><b>IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning Datasets
                      for Indian Languages</b></div>
                  <div class="author">
                    <font size="-1">
                      Mohammed Safi Ur Rahman Khan, Priyam Mehta, Ananth Sankar, Umashankar Kumaravelan, <span
                        style="color:cadetblue">Sumanth
                        Doddapaneni</span>, Suriyaprasaad G, Varun Balan G, Sparsh Jain, Anoop Kunchukuttan, Pratyush
                      Kumar, Raj Dabre, Mitesh M. Khapra
                    </font>
                  </div>
                  <div class="links abbr">
                    <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a
                      href="https://arxiv.org/abs/2403.06350" target="_blank"><i class="fas fa-file"></i></a> <a
                      href="https://github.com/AI4Bharat/IndicLLMSuite" target="_blank"><i
                        class="fab fa-github"></i></a>
                    | 🏆 Outstanding Paper
                  </div>

                  <div class="abstract hidden">
                    <p>Despite the considerable advancements in English LLMs, the progress in building comparable models
                      for other languages has been hindered due to the scarcity of tailored resources. Our work aims to
                      bridge this divide by introducing an expansive suite of resources specifically designed for the
                      development of Indic LLMs, covering 22 languages, containing a total of 251B tokens and 74.8M
                      instruction-response pairs. Recognizing the importance of both data quality and quantity, our
                      approach combines highly curated manually verified data, unverified yet valuable data, and
                      synthetic data. We build a clean, open-source pipeline for curating pre-training data from diverse
                      sources, including websites, PDFs, and videos, incorporating best practices for crawling,
                      cleaning, flagging, and deduplication. For instruction-fine tuning, we amalgamate existing Indic
                      datasets, translate/transliterate English datasets into Indian languages, and utilize LLaMa2 and
                      Mixtral models to create conversations grounded in articles from Indian Wikipedia and Wikihow.
                      Additionally, we address toxicity alignment by generating toxic prompts for multiple scenarios and
                      then generate non-toxic responses by feeding these toxic prompts to an aligned LLaMa2 model. We
                      hope that the datasets, tools, and resources released as a part of this work will not only propel
                      the research and development of Indic LLMs but also establish an open-source blueprint for
                      extending such efforts to other languages. The data and other artifacts created as part of this
                      work are released with permissive licenses.
                    </p>
                  </div>
                </div>
            </li>
          </ol>


          <h2 class="year">2023</h2>
          <ol class="bibliography">

            <li>
              <div class="row">

                <div class="col-sm-2 abbr">
                  <abbr class="badge">TMLR</abbr>
                </div>

                <div id="ai4bharat2023indictrans2" class="col-sm-8">
                  <div class="title"><b>IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for
                      all 22 Scheduled Indian Languages</b></div>
                  <div class="author">
                    <font size="-1">
                      AI4Bharat, Jay Gala, Pranjal A. Chitale, Raghavan AK, <span style="color:cadetblue">Sumanth
                        Doddapaneni</span>, Varun Gumma, Aswanth Kumar, Janki Nawale, Anupama Sujatha, Ratish
                      Puduppully, Vivek Raghavan, Pratyush Kumar, Mitesh M. Khapra, Raj Dabre, Anoop Kunchukuttan
                    </font>
                  </div>
                  <div class="links abbr">
                    <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a
                      href="https://arxiv.org/abs/2305.16307" target="_blank"><i class="fas fa-file"></i></a> <a
                      href="https://github.com/ai4bharat/IndicTrans2" target="_blank"><i class="fab fa-github"></i></a>
                  </div>

                  <div class="abstract hidden">
                    <p>India has a rich linguistic landscape with languages from 4 major language families spoken by
                      over a billion people. 22 of these languages are listed in the Constitution of India (referred to
                      as scheduled languages) are the focus of this work. Given the linguistic diversity, high-quality
                      and accessible Machine Translation (MT) systems are essential in a country like India. Prior to
                      this work, there was (i) no parallel training data spanning all the 22 languages, (ii) no robust
                      benchmarks covering all these languages and containing content relevant to India, and (iii) no
                      existing translation models which support all the 22 scheduled languages of India. In this work,
                      we aim to address this gap by focusing on the missing pieces required for enabling wide, easy, and
                      open access to good machine translation systems for all 22 scheduled Indian languages. We identify
                      four key areas of improvement: curating and creating larger training datasets, creating diverse
                      and high-quality benchmarks, training multilingual models, and releasing models with open access.
                      Our first contribution is the release of the Bharat Parallel Corpus Collection (BPCC), the largest
                      publicly available parallel corpora for Indic languages. BPCC contains a total of 230M bitext
                      pairs, of which a total of 126M were newly added, including 644K manually translated sentence
                      pairs created as part of this work. Our second contribution is the release of the first n-way
                      parallel benchmark covering all 22 Indian languages, featuring diverse domains, Indian-origin
                      content, and source-original test sets. Next, we present IndicTrans2, the first model to support
                      all 22 languages, surpassing existing models on multiple existing and new benchmarks created as a
                      part of this work. Lastly, to promote accessibility and collaboration, we release our models and
                      associated data with permissive licenses at https://github.com/ai4bharat/IndicTrans2.
                    </p>
                  </div>
                </div>
            </li>

            <li>
              <div class="row">

                <div class="col-sm-2 abbr">
                  <abbr class="badge">CODS-COMAD</abbr>
                </div>

                <div id="mundra2023a" class="col-sm-8">
                  <div class="title"><b>A Comprehensive Analysis of Adapter Efficiency</b></div>
                  <div class="author">
                    <font size="-1">
                      Nandini Mundra, <span style="color:cadetblue">Sumanth Doddapaneni</span>, Raj Dabre, Anoop
                      Kunchukuttan, Ratish Puduppully, Mitesh M. Khapra
                    </font>
                  </div>
                  <div class="links abbr">
                    <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a
                      href="https://arxiv.org/abs/2305.07491" target="_blank"><i class="fas fa-file"></i></a>
                  </div>

                  <div class="abstract hidden">
                    <p>Adapters have been positioned as a parameter-efficient fine-tuning (PEFT) approach, whereby a
                      minimal number of parameters are added to the model and fine-tuned. However, adapters have not
                      been sufficiently analyzed to understand if PEFT translates to benefits in training/deployment
                      efficiency and maintainability/extensibility. Through extensive experiments on many adapters,
                      tasks, and languages in supervised and cross-lingual zero-shot settings, we clearly show that for
                      Natural Language Understanding (NLU) tasks, the parameter efficiency in adapters does not
                      translate to efficiency gains compared to full fine-tuning of models. More precisely, adapters are
                      relatively expensive to train and have slightly higher deployment latency. Furthermore, the
                      maintainability/extensibility benefits of adapters can be achieved with simpler approaches like
                      multi-task training via full fine-tuning, which also provide relatively faster training times. We,
                      therefore, recommend that for moderately sized models for NLU tasks, practitioners should rely on
                      full fine-tuning or multi-task training rather than using adapters. Our code is available at
                      https://github.com/AI4Bharat/adapter-efficiency</p>
                  </div>
                </div>
            </li>

            <li>
              <div class="row">

                <div class="col-sm-2 abbr">
                  <abbr class="badge">ACL 2023</abbr>
                </div>

                <div id="aralikatte2023varta" class="col-sm-8">
                  <div class="title"><b>Vārta: A Large-Scale Headline-Generation Dataset for Indic Languages</b></div>
                  <div class="author">
                    <font size="-1">
                      Rahul Aralikatte, Ziling Cheng, <span style="color:cadetblue">Sumanth Doddapaneni</span>, Jackie
                      Chi Kit Cheung
                    </font>
                  </div>
                  <div class="links abbr">
                    <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a
                      href="https://arxiv.org/abs/2305.05858" target="_blank"><i class="fas fa-file"></i></a> <a
                      href="https://github.com/rahular/varta" target="_blank"><i class="fab fa-github"></i></a>
                  </div>

                  <div class="abstract hidden">
                    <p>We present Vārta, a large-scale multilingual dataset for headline generation in Indic languages.
                      This dataset includes 41.8 million news articles in 14 different Indic languages (and English),
                      which come from a variety of high-quality sources. To the best of our knowledge, this is the
                      largest collection of curated articles for Indic languages currently available. We use the data
                      collected in a series of experiments to answer important questions related to Indic NLP and
                      multilinguality research in general. We show that the dataset is challenging even for
                      state-of-the-art abstractive models and that they perform only slightly better than extractive
                      baselines. Owing to its size, we also show that the dataset can be used to pretrain strong
                      language models that outperform competitive baselines in both NLU and NLG benchmarks.</p>
                  </div>
                </div>
            </li>


            <li>
              <div class="row">

                <div class="col-sm-2 abbr">
                  <abbr class="badge">ACL 2023</abbr>
                </div>

                <div id="mhaske2022naamapadam" class="col-sm-8">
                  <div class="title"><b>Naamapadam: A Large-Scale Named Entity Annotated Data for Indic Languages</b>
                  </div>
                  <div class="author">
                    <font size="-1">
                      Arnav Mhaske, Harshit Kedia, <span style="color:cadetblue">Sumanth Doddapaneni</span>, Mitesh M.
                      Khapra, Pratyush Kumar, Rudra Murthy V, Anoop Kunchukuttan
                    </font>
                  </div>
                  <div class="links abbr">
                    <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a
                      href="https://arxiv.org/abs/2212.10168" target="_blank"><i class="fas fa-file"></i></a>
                  </div>

                  <div class="abstract hidden">
                    <p>We present, Naamapadam, the largest publicly available Named Entity Recognition (NER) dataset for
                      the 11 major Indian languages from two language families. In each language, it contains more than
                      400k sentences annotated with a total of at least 100k entities from three standard entity
                      categories (Person, Location and Organization) for 9 out of the 11 languages. The training dataset
                      has been automatically created from the Samanantar parallel corpus by projecting automatically
                      tagged entities from an English sentence to the corresponding Indian language sentence. We also
                      create manually annotated testsets for 8 languages containing approximately 1000 sentences per
                      language. We demonstrate the utility of the obtained dataset on existing testsets and the
                      Naamapadam-test data for 8 Indic languages. We also release IndicNER, a multilingual mBERT model
                      fine-tuned on the Naamapadam training set. IndicNER achieves the best F1 on the Naamapadam-test
                      set compared to an mBERT model fine-tuned on existing datasets. IndicNER achieves an F1 score of
                      more than 80 for 7 out of 11 Indic languages. The dataset and models are available under
                      open-source licenses at https://ai4bharat.iitm.ac.in/naamapadam.</p>
                  </div>

                </div>
              </div>
            </li>

            <li>
              <div class="row">

                <div class="col-sm-2 abbr">
                  <abbr class="badge">ACL 2023</abbr>
                </div>

                <div id="doddapanei2022indicxtreme" class="col-sm-8">
                  <div class="title"><b>Towards Leaving No Indic Language Behind: Building Monolingual Corpora,
                      Benchmark and Models for Indic Languages</b></div>
                  <div class="author">
                    <font size="-1">
                      <span style="color:cadetblue">Sumanth Doddapaneni</span>, Rahul Aralikatte, Gowtham Ramesh, Shreya
                      Goyal, Mitesh M. Khapra, Anoop Kunchukuttan, Pratyush Kumar
                    </font>
                  </div>
                  <div class="links abbr">
                    <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a
                      href="https://arxiv.org/abs/2212.05409" target="_blank"><i class="fas fa-file"></i></a> <a
                      href="https://github.com/AI4Bharat/IndicBERT" target="_blank"><i class="fab fa-github"></i></a>
                  </div>

                  <div class="abstract hidden">
                    <p>Building Natural Language Understanding (NLU) capabilities for Indic languages, which have a
                      collective speaker base of more than one billion speakers is absolutely crucial. In this work, we
                      aim to improve the NLU capabilities of Indic languages by making contributions along 3 important
                      axes (i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing on Indic
                      languages. Specifically, we curate the largest monolingual corpora, IndicCorp, with 20.9B tokens
                      covering 24 languages from 4 language families - a 2.3x increase over prior work, while supporting
                      12 additional languages. Next, we create a human-supervised benchmark, IndicXTREME, consisting of
                      nine diverse NLU tasks covering 20 languages. Across languages and tasks, IndicXTREME contains a
                      total of 105 evaluation sets, of which 52 are new contributions to the literature. To the best of
                      our knowledge, this is the first effort towards creating a standard benchmark for Indic languages
                      that aims to test the multilingual zero-shot capabilities of pretrained language models. Finally,
                      we train IndicBERT v2, a state-of-the-art model supporting all the languages. Averaged across
                      languages and tasks, the model achieves an absolute improvement of 2 points over a strong
                      baseline. The data and models are available at \url{https://github.com/AI4Bharat/IndicBERT}.</p>
                  </div>

                </div>
              </div>
            </li>
          </ol>

          <h2 class="year">2022</h2>
          <ol class="bibliography">
            <li>
              <div class="row">

                <div class="col-sm-2 abbr">
                  <abbr class="badge">ICASSP</abbr>
                </div>

                <div id="bhogale2022effectiveness" class="col-sm-8">
                  <div class="title"><a href="https://arxiv.org/abs/2208.12666" target="_blank"><b>Effectiveness of
                        Mining Audio and Text Pairs from Public Data for Improving ASR Systems for Low-Resource
                        Languages</b></a></div>
                  <div class="author">
                    Kaushal Santosh Bhogale,
                    Abhigyan Raman,
                    Tahir Javed,
                    <span style="color:cadetblue">Sumanth Doddapaneni</span>,
                    Anoop Kunchukuttan,
                    Pratyush Kumar,
                    Mitesh M. Khapra,
                  </div>
                  <div class="periodical">2022</div>
                  <div class="links abbr">
                    <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
                  </div>

                  <div class="abstract hidden">
                    <p>End-to-end (E2E) models have become the default choice for state-of-the-art speech recognition
                      systems. Such models are trained on large amounts of labelled data, which are often not available
                      for low-resource languages. Techniques such as self-supervised learning and transfer learning hold
                      promise, but have not yet been effective in training accurate models. On the other hand,
                      collecting labelled datasets on a diverse set of domains and speakers is very expensive. In this
                      work, we demonstrate an inexpensive and effective alternative to these approaches by ``mining''
                      text and audio pairs for Indian languages from public sources, specifically from the public
                      archives of All India Radio. As a key component, we adapt the Needleman-Wunsch algorithm to align
                      sentences with corresponding audio segments given a long audio and a PDF of its transcript, while
                      being robust to errors due to OCR, extraneous text, and non-transcribed speech. We thus create
                      Shrutilipi, a dataset which contains over 6,400 hours of labelled audio across 12 Indian languages
                      totalling to 4.95M sentences. On average, Shrutilipi results in a 2.3x increase over publicly
                      available labelled data. We establish the quality of Shrutilipi with 21 human evaluators across
                      the 12 languages. We also establish the diversity of Shrutilipi in terms of represented regions,
                      speakers, and mentioned named entities. Significantly, we show that adding Shrutilipi to the
                      training set of Wav2Vec models leads to an average decrease in WER of 5.8\% for 7 languages on the
                      IndicSUPERB benchmark. For Hindi, which has the most benchmarks (7), the average WER falls from
                      18.8% to 13.5%. This improvement extends to efficient models: We show a 2.3% drop in WER for a
                      Conformer model (10x smaller than Wav2Vec). Finally, we demonstrate the diversity of Shrutilipi by
                      showing that the model trained with it is more robust to noisy input.</p>
                  </div>

                </div>
              </div>
            </li>

            <li>
              <div class="row">

                <div class="col-sm-2 abbr">
                  <abbr class="badge">AAAI</abbr>
                </div>

                <div id="javed2022towards" class="col-sm-8">
                  <div class="title"><a href="https://ojs.aaai.org/index.php/AAAI/article/view/21327"
                      target="_blank"><b>Towards Building ASR Systems For The Next Billion Users</b></a></div>
                  <div class="author">
                    Tahir Javed,
                    <span style="color:cadetblue">Sumanth Doddapaneni</span>,
                    Abhigyan Raman,
                    Kaushal Santosh Bhogale,
                    Gowtham Ramesh,
                    Anoop Kunchukuttan,
                    Pratyush Kumar,
                    Mitesh M. Khapra,
                  </div>
                  <div class="periodical">2022</div>
                  <div class="links abbr">
                    <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a
                      href="https://ojs.aaai.org/index.php/AAAI/article/view/21327"><i class="fas fa-file"></i></a> <a
                      href="https://github.com/AI4Bharat/indicwav2vec"><i class="fab fa-github"></i></a>
                  </div>

                  <div class="abstract hidden">
                    <p>Recent methods in speech and language technology pretrain very large models which are fine-tuned
                      for specific tasks. However, the benefits of such large models are often limited to a few resource
                      rich languages of the world. In this work, we make multiple contributions towards building ASR
                      systems for low resource languages from the Indian subcontinent. First, we curate 17,000 hours of
                      raw speech data for 40 Indian languages from a wide variety of domains including education, news,
                      technology, and finance. Second, using this raw speech data we pretrain several variants of
                      wav2vec style models for 40 Indian languages. Third, we analyze the pretrained models to find key
                      features: codebook vectors of similar sounding phonemes are shared across languages,
                      representations across layers are discriminative of the language family, and attention heads often
                      pay attention within small local windows. Fourth, we fine-tune this model for downstream ASR for 9
                      languages and obtain state-of-the-art results on 3 public datasets, including on very low-resource
                      languages such as Sinhala and Nepali. Our work establishes that multilingual pretraining is an
                      effective strategy for building ASR systems for the linguistically diverse speakers of the Indian
                      subcontinent.</p>
                  </div>

                </div>
              </div>
            </li>

            <li>
              <div class="row">

                <div class="col-sm-2 abbr">
                  <abbr class="badge">ACM CSUR</abbr>
                </div>

                <div id="javed2022towards" class="col-sm-8">
                  <div class="title"><a href="https://arxiv.org/abs/2203.06414" target="_blank"><b>A Survey in
                        Adversarial Defences and Robustness in NLP</b></a></div>
                  <div class="author">
                    Shreya Goyal,
                    <span style="color:cadetblue">Sumanth Doddapaneni</span>,
                    Mitesh M. Khapra,
                    Balaraman Ravindran
                  </div>
                  <div class="periodical">2022</div>
                  <div class="links abbr">
                    <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
                  </div>

                  <div class="abstract hidden">
                    <p>In recent years, it has been seen that deep neural networks are lacking robustness and are likely
                      to break in case of adversarial perturbations in input data. Strong adversarial attacks are
                      proposed by various authors for computer vision and Natural Language Processing (NLP). As a
                      counter-effort, several defense mechanisms are also proposed to save these networks from failing.
                      In contrast with image data, generating adversarial attacks and defending these models is not easy
                      in NLP because of the discrete nature of the text data. However, numerous methods for adversarial
                      defense are proposed of late, for different NLP tasks such as text classification, named entity
                      recognition, natural language inferencing, etc. These methods are not just used for defending
                      neural networks from adversarial attacks, but also used as a regularization mechanism during
                      training, saving the model from overfitting. The proposed survey is an attempt to review different
                      methods proposed for adversarial defenses in NLP in the recent past by proposing a novel taxonomy.
                      This survey also highlights the fragility of the advanced deep neural networks in NLP and the
                      challenges in defending them.</p>
                  </div>

                </div>
              </div>
            </li>

            <li>
              <div class="row">

                <div class="col-sm-2 abbr">
                  <abbr class="badge">TACL</abbr>
                </div>

                <div id="javed2022towards" class="col-sm-8">
                  <div class="title"><a
                      href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00452/109468/Samanantar-The-Largest-Publicly-Available-Parallel"
                      target="_blank"><b>Samanantar: The largest publicly available parallel corpora collection for 11
                        indic languages</b></a></div>
                  <div class="author">
                    Gowtham Ramesh*, <span style="color:cadetblue">Sumanth Doddapaneni*</span>, Aravinth Bheemaraj,
                    Mayank Jobanputra, Raghavan AK, Ajitesh Sharma, Sujit Sahoo, Harshita Diddee, Divyanshu Kakwani,
                    Navneet Kumar, Aswin Pradeep, Srihari Nagaraj, Kumar Deepak, Vivek Raghavan, Anoop Kunchukuttan,
                    Pratyush Kumar, Mitesh Shantadevi Khapra
                  </div>
                  <div class="periodical">2022</div>
                  <div class="links abbr">
                    <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a
                      href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00452/109468/Samanantar-The-Largest-Publicly-Available-Parallel"><i
                        class="fas fa-file"></i></a> <a href="https://github.com/AI4Bharat/indicTrans"><i
                        class="fab fa-github"></i></a>
                  </div>

                  <div class="abstract hidden">
                    <p>We present Samanantar, the largest publicly available parallel corpora collection for Indic
                      languages. The collection contains a total of 49.7 million sentence pairs between English and 11
                      Indic languages (from two language families). Specifically, we compile 12.4 million sentence pairs
                      from existing, publicly available parallel corpora, and additionally mine 37.4 million sentence
                      pairs from the Web, resulting in a 4× increase. We mine the parallel sentences from the Web by
                      combining many corpora, tools, and methods: (a) Web-crawled monolingual corpora, (b) document OCR
                      for extracting sentences from scanned documents, (c) multilingual representation models for
                      aligning sentences, and (d) approximate nearest neighbor search for searching in a large
                      collection of sentences. Human evaluation of samples from the newly mined corpora validate the
                      high quality of the parallel sentences across 11 languages. Further, we extract 83.4 million
                      sentence pairs between all 55 Indic language pairs from the English-centric parallel corpus using
                      English as the pivot language. We trained multilingual NMT models spanning all these languages on
                      Samanantar which outperform existing models and baselines on publicly available benchmarks, such
                      as FLORES, establishing the utility of Samanantar. Our data and models are available publicly at
                      Samanantar and we hope they will help advance research in NMT and multilingual NLP for Indic
                      languages.</p>
                  </div>

                </div>
              </div>
            </li>

            <li>
              <div class="row">

                <div class="col-sm-2 abbr">
                  <abbr class="badge">arXiv</abbr>
                </div>

                <div id="javed2022towards" class="col-sm-8">
                  <div class="title"><a href="https://arxiv.org/abs/2203.06414" target="_blank"><b>Offence Detection in
                        Dravidian Languages Using Code-Mixing Index-Based Focal Loss</b></a></div>
                  <div class="author">
                    Debapriya Tula,
                    MS Shreyas,
                    Viswanatha Reddy,
                    Pranjal Sahu,
                    <span style="color:cadetblue">Sumanth Doddapaneni</span>,
                    Prathyush Potluri,
                    Rohan Sukumaran,
                    Parth Patwa
                  </div>
                  <div class="periodical">2022</div>
                  <div class="links abbr">
                    <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
                  </div>

                  <div class="abstract hidden">
                    <p>Over the past decade, we have seen exponential growth in online content fueled by social media
                      platforms. Data generation of this scale comes with the caveat of insurmountable offensive content
                      in it. The complexity of identifying offensive content is exacerbated by the usage of multiple
                      modalities (image, language, etc.), code mixed language and more. Moreover, even after careful
                      sampling and annotation of offensive content, there will always exist a significant class
                      imbalance between offensive and non-offensive content. In this paper, we introduce a novel
                      code-mixing index (CMI) based focal loss which circumvents two challenges (1) code-mixing in
                      languages (2) class imbalance problem for Dravidian language offence detection. We also replace
                      the conventional dot product-based classifier with the cosine-based classifier which results in a
                      boost in performance. Further, we use multilingual models that help transfer characteristics
                      learnt across languages to work effectively with low-resourced languages. It is also important to
                      note that our model handles instances of mixed script (say usage of Latin and Dravidian—Tamil
                      script) as well. To summarize, our model can handle offensive language detection in a
                      low-resource, class imbalanced, multilingual and code mixed setting. The code is publicly
                      available at https://github.com/Debapriya-Tula/EACL2021-DravidianTask-Bitions.</p>
                  </div>

                </div>
              </div>
            </li>
          </ol>


          <h2 class="year">2021</h2>
          <ol class="bibliography">
            <li>
              <div class="row">

                <div class="col-sm-2 abbr">
                  <abbr class="badge">ACM CSUR</abbr>
                </div>

                <div id="javed2022towards" class="col-sm-8">
                  <div class="title"><a href="https://arxiv.org/abs/2107.00676" target="_blank"><b>A Primer on
                        Pretrained Multilingual Language Models</b></a></div>
                  <div class="author">
                    <span style="color:cadetblue">Sumanth Doddapaneni*</span>, Gowtham Ramesh*, Anoop Kunchukuttan,
                    Pratyush Kumar, Mitesh M Khapra
                  </div>
                  <div class="periodical">2021</div>
                  <div class="links abbr">
                    <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
                  </div>

                  <div class="abstract hidden">
                    <p>Multilingual Language Models (\MLLMs) such as mBERT, XLM, XLM-R, \textit{etc.} have emerged as a
                      viable option for bringing the power of pretraining to a large number of languages. Given their
                      success in zero-shot transfer learning, there has emerged a large body of work in (i) building
                      bigger \MLLMs~covering a large number of languages (ii) creating exhaustive benchmarks covering a
                      wider variety of tasks and languages for evaluating \MLLMs~ (iii) analysing the performance of
                      \MLLMs~on monolingual, zero-shot cross-lingual and bilingual tasks (iv) understanding the
                      universal language patterns (if any) learnt by \MLLMs~ and (v) augmenting the (often) limited
                      capacity of \MLLMs~ to improve their performance on seen or even unseen languages. In this survey,
                      we review the existing literature covering the above broad areas of research pertaining to \MLLMs.
                      Based on our survey, we recommend some promising directions of future research.</p>
                  </div>

                </div>
              </div>
            </li>

            <li>
              <div class="row">

                <div class="col-sm-2 abbr">
                  <abbr class="badge">EACL-W</abbr>
                </div>

                <div id="javed2022towards" class="col-sm-8">
                  <div class="title"><a href="https://arxiv.org/abs/2107.00676" target="_blank"><b>Bitions@
                        DravidianLangTech-EACL2021: Ensemble of multilingual language models with pseudo labeling for
                        offence detection in Dravidian languages</b></a></div>
                  <div class="author">
                    Debapriya Tula, Prathyush Potluri, Shreyas Ms, <span style="color:cadetblue">Sumanth
                      Doddapaneni</span>, Pranjal Sahu, Rohan Sukumaran, Parth Patwa
                  </div>
                  <div class="periodical">2021</div>
                  <div class="links abbr">
                    <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
                  </div>

                  <div class="abstract hidden">
                    <p>With the advent of social media, we have seen a proliferation of data and public discourse.
                      Unfortunately, this includes offensive content as well. The problem is exacerbated due to the
                      sheer number of languages spoken on these platforms and the multiple other modalities used for
                      sharing offensive content (images, gifs, videos and more). In this paper, we propose a
                      multilingual ensemble-based model that can identify offensive content targeted against an
                      individual (or group) in low resource Dravidian language. Our model is able to handle code-mixed
                      data as well as instances where the script used is mixed (for instance, Tamil and Latin). Our
                      solution ranked number one for the Malayalam dataset and ranked 4th and 5th for Tamil and Kannada,
                      respectively.</p>
                  </div>

                </div>
              </div>
            </li>
          </ol>

        </div>

      </article>

    </div>

  </div>

  <!-- Footer -->


  <footer class="fixed-bottom">
    <div class="container mt-0">
      &copy; Copyright 2024 Sumanth Doddapaneni. All rights reserved.
    </div>
  </footer>



</body>

<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"
  integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg=="
  crossorigin="anonymous"></script>

<!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js"
  integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A=="
  crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"
  integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ=="
  crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js"
  integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw=="
  crossorigin="anonymous"></script>


<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>






<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>