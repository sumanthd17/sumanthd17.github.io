<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Sumanth Doddapaneni | publications</title>
<meta name="description" content="">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">

<!-- Theming-->


    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Sumanth</span> 
      </a>
      
      
        <!-- Social Icons -->
        <div class="row ml-1 ml-sm-0">
          <span class="contact-icon text-center">
        <a href="mailto:doddapaneni.sumanth@gmail.com"><i class="fas fa-envelope"></i></a>

        <a href="https://scholar.google.com/citations?user=_kR4rGAAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
        <a href="https://semanticscholar.com/author/2072738714" target="_blank" title="Semantic Scholar"><i class="ai ai-semantic-scholar"></i></a>
        
        
        <a href="https://github.com/sumanthd17" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
        
        <a href="https://twitter.com/sumanthd17" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>
  
  
  
  
  
</span>

        </div>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->

          <li class="nav-item ">
            <a class="nav-link" href="/publications/">
              publications
              
            </a>
          </li>

          <li class="nav-item ">
            <a class="nav-link" href="/assets/pdf/sumanthResume.pdf">
              resume
            </a>
          </li>
          
          <!-- <li class="nav-item ">
              <a class="nav-link" href="/mentoring/">
                mentoring
                
              </a>
          </li>

          <li class="nav-item ">
              <a class="nav-link" href="/prospective/">
                prospective
                
              </a>
          </li>

          <li class="nav-item ">
              <a class="nav-link" href="/talks/">
                talks
                
              </a>
          </li>

          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li> -->
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="desc">Also see <a href="https://scholar.google.com/citations?user=_kR4rGAAAAAJ">Google Scholar</a> and <a href="https://semanticscholar.com/author/2072738714">Semantic Scholar</a></p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2023</h2>
  <ol class="bibliography">

    <li>
      <div class="row">
  
      <div class="col-sm-2 abbr">
        <abbr class="badge">ArXiv 2023</abbr>
      </div>

        <div id="ai4bharat2023indictrans2" class="col-sm-8">
          <div class="title"><b>IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages</b></div>
        <div class="author">
          <font size="-1">
            AI4Bharat, Jay Gala, Pranjal A. Chitale, Raghavan AK, <span style="color:cadetblue">Sumanth Doddapaneni</span>, Varun Gumma, Aswanth Kumar, Janki Nawale, Anupama Sujatha, Ratish Puduppully, Vivek Raghavan, Pratyush Kumar, Mitesh M. Khapra, Raj Dabre, Anoop Kunchukuttan
        </font>
        </div>
      <div class="links abbr">
        <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://arxiv.org/abs/2305.16307" target="_blank"><i class="fas fa-file"></i></a> <a href="https://github.com/ai4bharat/IndicTrans2" target="_blank"><i class="fab fa-github"></i></a>
      </div>
    
      <div class="abstract hidden">
        <p>India has a rich linguistic landscape with languages from 4 major language families spoken by over a billion people. 22 of these languages are listed in the Constitution of India (referred to as scheduled languages) are the focus of this work. Given the linguistic diversity, high-quality and accessible Machine Translation (MT) systems are essential in a country like India. Prior to this work, there was (i) no parallel training data spanning all the 22 languages, (ii) no robust benchmarks covering all these languages and containing content relevant to India, and (iii) no existing translation models which support all the 22 scheduled languages of India. In this work, we aim to address this gap by focusing on the missing pieces required for enabling wide, easy, and open access to good machine translation systems for all 22 scheduled Indian languages. We identify four key areas of improvement: curating and creating larger training datasets, creating diverse and high-quality benchmarks, training multilingual models, and releasing models with open access. Our first contribution is the release of the Bharat Parallel Corpus Collection (BPCC), the largest publicly available parallel corpora for Indic languages. BPCC contains a total of 230M bitext pairs, of which a total of 126M were newly added, including 644K manually translated sentence pairs created as part of this work. Our second contribution is the release of the first n-way parallel benchmark covering all 22 Indian languages, featuring diverse domains, Indian-origin content, and source-original test sets. Next, we present IndicTrans2, the first model to support all 22 languages, surpassing existing models on multiple existing and new benchmarks created as a part of this work. Lastly, to promote accessibility and collaboration, we release our models and associated data with permissive licenses at https://github.com/ai4bharat/IndicTrans2.
        </p>
      </div>
      </div>
    </li>

    <li>
      <div class="row">
  
      <div class="col-sm-2 abbr">
        <abbr class="badge">ArXiv</abbr>
      </div>

        <div id="mundra2023a" class="col-sm-8">
          <div class="title"><b>A Comprehensive Analysis of Adapter Efficiency</b></div>
        <div class="author">
          <font size="-1">
            Nandini Mundra, <span style="color:cadetblue">Sumanth Doddapaneni</span>, Raj Dabre, Anoop Kunchukuttan, Ratish Puduppully, Mitesh M. Khapra
        </font>
        </div>
      <div class="links abbr">
        <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://arxiv.org/abs/2305.07491" target="_blank"><i class="fas fa-file"></i></a>
      </div>
    
      <div class="abstract hidden">
        <p>Adapters have been positioned as a parameter-efficient fine-tuning (PEFT) approach, whereby a minimal number of parameters are added to the model and fine-tuned. However, adapters have not been sufficiently analyzed to understand if PEFT translates to benefits in training/deployment efficiency and maintainability/extensibility. Through extensive experiments on many adapters, tasks, and languages in supervised and cross-lingual zero-shot settings, we clearly show that for Natural Language Understanding (NLU) tasks, the parameter efficiency in adapters does not translate to efficiency gains compared to full fine-tuning of models. More precisely, adapters are relatively expensive to train and have slightly higher deployment latency. Furthermore, the maintainability/extensibility benefits of adapters can be achieved with simpler approaches like multi-task training via full fine-tuning, which also provide relatively faster training times. We, therefore, recommend that for moderately sized models for NLU tasks, practitioners should rely on full fine-tuning or multi-task training rather than using adapters. Our code is available at https://github.com/AI4Bharat/adapter-efficiency</p>
      </div>
      </div>
    </li>

    <li>
      <div class="row">
  
      <div class="col-sm-2 abbr">
        <abbr class="badge">ACL 2023</abbr>
      </div>

        <div id="aralikatte2023varta" class="col-sm-8">
          <div class="title"><b>Vārta: A Large-Scale Headline-Generation Dataset for Indic Languages</b></div>
        <div class="author">
          <font size="-1">
            Rahul Aralikatte, Ziling Cheng, <span style="color:cadetblue">Sumanth Doddapaneni</span>, Jackie Chi Kit Cheung
        </font>
        </div>
      <div class="links abbr">
        <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://arxiv.org/abs/2305.05858" target="_blank"><i class="fas fa-file"></i></a> <a href="https://github.com/rahular/varta" target="_blank"><i class="fab fa-github"></i></a>
      </div>
    
      <div class="abstract hidden">
        <p>We present Vārta, a large-scale multilingual dataset for headline generation in Indic languages. This dataset includes 41.8 million news articles in 14 different Indic languages (and English), which come from a variety of high-quality sources. To the best of our knowledge, this is the largest collection of curated articles for Indic languages currently available. We use the data collected in a series of experiments to answer important questions related to Indic NLP and multilinguality research in general. We show that the dataset is challenging even for state-of-the-art abstractive models and that they perform only slightly better than extractive baselines. Owing to its size, we also show that the dataset can be used to pretrain strong language models that outperform competitive baselines in both NLU and NLG benchmarks.</p>
      </div>
      </div>
    </li>


    <li>
      <div class="row">
  
      <div class="col-sm-2 abbr">
        <abbr class="badge">ACL 2023</abbr>
      </div>
  
      <div id="mhaske2022naamapadam" class="col-sm-8">
            <div class="title"><b>Naamapadam: A Large-Scale Named Entity Annotated Data for Indic Languages</b></div>
          <div class="author">
            <font size="-1">
              Arnav Mhaske, Harshit Kedia, <span style="color:cadetblue">Sumanth Doddapaneni</span>, Mitesh M. Khapra, Pratyush Kumar, Rudra Murthy V, Anoop Kunchukuttan
          </font>
          </div>
        <div class="links abbr">
          <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://arxiv.org/abs/2212.10168" target="_blank"><i class="fas fa-file"></i></a>
        </div>
       
        <div class="abstract hidden">
          <p>We present, Naamapadam, the largest publicly available Named Entity Recognition (NER) dataset for the 11 major Indian languages from two language families. In each language, it contains more than 400k sentences annotated with a total of at least 100k entities from three standard entity categories (Person, Location and Organization) for 9 out of the 11 languages. The training dataset has been automatically created from the Samanantar parallel corpus by projecting automatically tagged entities from an English sentence to the corresponding Indian language sentence. We also create manually annotated testsets for 8 languages containing approximately 1000 sentences per language. We demonstrate the utility of the obtained dataset on existing testsets and the Naamapadam-test data for 8 Indic languages. We also release IndicNER, a multilingual mBERT model fine-tuned on the Naamapadam training set. IndicNER achieves the best F1 on the Naamapadam-test set compared to an mBERT model fine-tuned on existing datasets. IndicNER achieves an F1 score of more than 80 for 7 out of 11 Indic languages. The dataset and models are available under open-source licenses at https://ai4bharat.iitm.ac.in/naamapadam.</p>
        </div>
        
      </div>
      </div>
    </li>

    <li>
      <div class="row">
  
      <div class="col-sm-2 abbr">
        <abbr class="badge">ACL 2023</abbr>
      </div>
  
      <div id="doddapanei2022indicxtreme" class="col-sm-8">
            <div class="title"><b>Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages</b></div>
          <div class="author">
            <font size="-1">
            <span style="color:cadetblue">Sumanth Doddapaneni</span>, Rahul Aralikatte, Gowtham Ramesh, Shreya Goyal, Mitesh M. Khapra, Anoop Kunchukuttan, Pratyush Kumar
          </font>
          </div>
        <div class="links abbr">
          <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://arxiv.org/abs/2212.05409" target="_blank"><i class="fas fa-file"></i></a> <a href="https://github.com/AI4Bharat/IndicBERT" target="_blank"><i class="fab fa-github"></i></a>
        </div>
       
        <div class="abstract hidden">
          <p>Building Natural Language Understanding (NLU) capabilities for Indic languages, which have a collective speaker base of more than one billion speakers is absolutely crucial. In this work, we aim to improve the NLU capabilities of Indic languages by making contributions along 3 important axes (i) monolingual corpora (ii) NLU testsets  (iii) multilingual LLMs focusing on Indic languages. Specifically, we curate the largest monolingual corpora, IndicCorp, with 20.9B tokens covering 24 languages from 4 language families - a 2.3x increase over prior work, while  supporting 12 additional languages. Next, we create a human-supervised benchmark, IndicXTREME, consisting of nine diverse NLU tasks covering 20 languages. Across languages and tasks, IndicXTREME contains a total of 105 evaluation sets, of which 52 are new contributions to the literature. To the best of our knowledge, this is the first effort towards creating a standard benchmark for Indic languages that aims to test the multilingual zero-shot capabilities of pretrained language models. Finally, we train IndicBERT v2, a state-of-the-art model supporting all the languages. Averaged across languages and tasks, the model achieves an absolute improvement of 2 points over a strong baseline. The data and models are available at \url{https://github.com/AI4Bharat/IndicBERT}.</p>
        </div>
        
      </div>
      </div>
    </li>
  </ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography">
    <li>
      <div class="row">
  
      <div class="col-sm-2 abbr">
        <abbr class="badge">ICASSP</abbr>
      </div>
  
      <div id="bhogale2022effectiveness" class="col-sm-8">
            <div class="title"><a href="https://arxiv.org/abs/2208.12666" target="_blank"><b>Effectiveness of Mining Audio and Text Pairs from Public Data for Improving ASR Systems for Low-Resource Languages</b></a></div>
          <div class="author">
              Kaushal Santosh Bhogale, 
              Abhigyan Raman, 
              Tahir Javed, 
              <span style="color:cadetblue">Sumanth Doddapaneni</span>, 
              Anoop Kunchukuttan, 
              Pratyush Kumar, 
              Mitesh M. Khapra,
          </div>
          <div class="periodical">2022</div>
        <div class="links abbr">
          <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
        </div>
       
        <div class="abstract hidden">
          <p>End-to-end (E2E) models have become the default choice for state-of-the-art speech recognition systems. Such models are trained on large amounts of labelled data, which are often not available for low-resource languages. Techniques such as self-supervised learning and transfer learning hold promise, but have not yet been effective in training accurate models. On the other hand, collecting labelled datasets on a diverse set of domains and speakers is very expensive. In this work, we demonstrate an inexpensive and effective alternative to these approaches by ``mining'' text and audio pairs for Indian languages from public sources, specifically from the public archives of All India Radio. As a key component, we adapt the Needleman-Wunsch algorithm to align sentences with corresponding audio segments given a long audio and a PDF of its transcript, while being robust to errors due to OCR, extraneous text, and non-transcribed speech. We thus create Shrutilipi, a dataset which contains over 6,400 hours of labelled audio across 12 Indian languages totalling to 4.95M sentences. On average, Shrutilipi results in a 2.3x increase over publicly available labelled data. We establish the quality of Shrutilipi with 21 human evaluators across the 12 languages. We also establish the diversity of Shrutilipi in terms of represented regions, speakers, and mentioned named entities. Significantly, we show that adding Shrutilipi to the training set of Wav2Vec models leads to an average decrease in WER of 5.8\% for 7 languages on the IndicSUPERB benchmark. For Hindi, which has the most benchmarks (7), the average WER falls from 18.8% to 13.5%. This improvement extends to efficient models: We show a 2.3% drop in WER for a Conformer model (10x smaller than Wav2Vec). Finally, we demonstrate the diversity of Shrutilipi by showing that the model trained with it is more robust to noisy input.</p>
        </div>
        
      </div>
      </div>
    </li>

    <li>
      <div class="row">
  
      <div class="col-sm-2 abbr">
        <abbr class="badge">AAAI</abbr>
      </div>
  
      <div id="javed2022towards" class="col-sm-8">
            <div class="title"><a href="https://ojs.aaai.org/index.php/AAAI/article/view/21327" target="_blank"><b>Towards Building ASR Systems For The Next Billion Users</b></a></div>
          <div class="author">
              Tahir Javed, 
              <span style="color:cadetblue">Sumanth Doddapaneni</span>, 
              Abhigyan Raman,
              Kaushal Santosh Bhogale,
              Gowtham Ramesh,
              Anoop Kunchukuttan, 
              Pratyush Kumar, 
              Mitesh M. Khapra,
          </div>
          <div class="periodical">2022</div>
        <div class="links abbr">
          <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/21327"><i class="fas fa-file"></i></a> <a href="https://github.com/AI4Bharat/indicwav2vec"><i class="fab fa-github"></i></a>
        </div>
       
        <div class="abstract hidden">
          <p>Recent methods in speech and language technology pretrain very large models which are fine-tuned for specific tasks. However, the benefits of such large models are often limited to a few resource rich languages of the world. In this work, we make multiple contributions towards building ASR systems for low resource languages from the Indian subcontinent. First, we curate 17,000 hours of raw speech data for 40 Indian languages from a wide variety of domains including education, news, technology, and finance. Second, using this raw speech data we pretrain several variants of wav2vec style models for 40 Indian languages. Third, we analyze the pretrained models to find key features: codebook vectors of similar sounding phonemes are shared across languages, representations across layers are discriminative of the language family, and attention heads often pay attention within small local windows. Fourth, we fine-tune this model for downstream ASR for 9 languages and obtain state-of-the-art results on 3 public datasets, including on very low-resource languages such as Sinhala and Nepali. Our work establishes that multilingual pretraining is an effective strategy for building ASR systems for the linguistically diverse speakers of the Indian subcontinent.</p>
        </div>
        
      </div>
      </div>
    </li>

    <li>
      <div class="row">
  
      <div class="col-sm-2 abbr">
        <abbr class="badge">ACM CSUR</abbr>
      </div>
  
      <div id="javed2022towards" class="col-sm-8">
            <div class="title"><a href="https://arxiv.org/abs/2203.06414" target="_blank"><b>A Survey in Adversarial Defences and Robustness in NLP</b></a></div>
          <div class="author">
              Shreya Goyal,
              <span style="color:cadetblue">Sumanth Doddapaneni</span>, 
              Mitesh M. Khapra,
              Balaraman Ravindran
          </div>
          <div class="periodical">2022</div>
        <div class="links abbr">
          <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
        </div>
       
        <div class="abstract hidden">
          <p>In recent years, it has been seen that deep neural networks are lacking robustness and are likely to break in case of adversarial perturbations in input data. Strong adversarial attacks are proposed by various authors for computer vision and Natural Language Processing (NLP). As a counter-effort, several defense mechanisms are also proposed to save these networks from failing. In contrast with image data, generating adversarial attacks and defending these models is not easy in NLP because of the discrete nature of the text data. However, numerous methods for adversarial defense are proposed of late, for different NLP tasks such as text classification, named entity recognition, natural language inferencing, etc. These methods are not just used for defending neural networks from adversarial attacks, but also used as a regularization mechanism during training, saving the model from overfitting. The proposed survey is an attempt to review different methods proposed for adversarial defenses in NLP in the recent past by proposing a novel taxonomy. This survey also highlights the fragility of the advanced deep neural networks in NLP and the challenges in defending them.</p>
        </div>
        
      </div>
      </div>
    </li>

    <li>
      <div class="row">
  
      <div class="col-sm-2 abbr">
        <abbr class="badge">TACL</abbr>
      </div>
  
      <div id="javed2022towards" class="col-sm-8">
            <div class="title"><a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00452/109468/Samanantar-The-Largest-Publicly-Available-Parallel" target="_blank"><b>Samanantar: The largest publicly available parallel corpora collection for 11 indic languages</b></a></div>
          <div class="author">
            Gowtham Ramesh*, <span style="color:cadetblue">Sumanth Doddapaneni*</span>, Aravinth Bheemaraj, Mayank Jobanputra, Raghavan AK, Ajitesh Sharma, Sujit Sahoo, Harshita Diddee, Divyanshu Kakwani, Navneet Kumar, Aswin Pradeep, Srihari Nagaraj, Kumar Deepak, Vivek Raghavan, Anoop Kunchukuttan, Pratyush Kumar, Mitesh Shantadevi Khapra
          </div>
          <div class="periodical">2022</div>
        <div class="links abbr">
          <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00452/109468/Samanantar-The-Largest-Publicly-Available-Parallel"><i class="fas fa-file"></i></a> <a href="https://github.com/AI4Bharat/indicTrans"><i class="fab fa-github"></i></a>
        </div>
       
        <div class="abstract hidden">
          <p>We present Samanantar, the largest publicly available parallel corpora collection for Indic languages. The collection contains a total of 49.7 million sentence pairs between English and 11 Indic languages (from two language families). Specifically, we compile 12.4 million sentence pairs from existing, publicly available parallel corpora, and additionally mine 37.4 million sentence pairs from the Web, resulting in a 4× increase. We mine the parallel sentences from the Web by combining many corpora, tools, and methods: (a) Web-crawled monolingual corpora, (b) document OCR for extracting sentences from scanned documents, (c) multilingual representation models for aligning sentences, and (d) approximate nearest neighbor search for searching in a large collection of sentences. Human evaluation of samples from the newly mined corpora validate the high quality of the parallel sentences across 11 languages. Further, we extract 83.4 million sentence pairs between all 55 Indic language pairs from the English-centric parallel corpus using English as the pivot language. We trained multilingual NMT models spanning all these languages on Samanantar which outperform existing models and baselines on publicly available benchmarks, such as FLORES, establishing the utility of Samanantar. Our data and models are available publicly at Samanantar and we hope they will help advance research in NMT and multilingual NLP for Indic languages.</p>
        </div>
        
      </div>
      </div>
    </li>

    <li>
      <div class="row">
  
      <div class="col-sm-2 abbr">
        <abbr class="badge">arXiv</abbr>
      </div>
  
      <div id="javed2022towards" class="col-sm-8">
            <div class="title"><a href="https://arxiv.org/abs/2203.06414" target="_blank"><b>Offence Detection in Dravidian Languages Using Code-Mixing Index-Based Focal Loss</b></a></div>
          <div class="author">
              Debapriya Tula, 
              MS Shreyas, 
              Viswanatha Reddy, 
              Pranjal Sahu,
              <span style="color:cadetblue">Sumanth Doddapaneni</span>, 
              Prathyush Potluri, 
              Rohan Sukumaran, 
              Parth Patwa
          </div>
          <div class="periodical">2022</div>
        <div class="links abbr">
          <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
        </div>
       
        <div class="abstract hidden">
          <p>Over the past decade, we have seen exponential growth in online content fueled by social media platforms. Data generation of this scale comes with the caveat of insurmountable offensive content in it. The complexity of identifying offensive content is exacerbated by the usage of multiple modalities (image, language, etc.), code mixed language and more. Moreover, even after careful sampling and annotation of offensive content, there will always exist a significant class imbalance between offensive and non-offensive content. In this paper, we introduce a novel code-mixing index (CMI) based focal loss which circumvents two challenges (1) code-mixing in languages (2) class imbalance problem for Dravidian language offence detection. We also replace the conventional dot product-based classifier with the cosine-based classifier which results in a boost in performance. Further, we use multilingual models that help transfer characteristics learnt across languages to work effectively with low-resourced languages. It is also important to note that our model handles instances of mixed script (say usage of Latin and Dravidian—Tamil script) as well. To summarize, our model can handle offensive language detection in a low-resource, class imbalanced, multilingual and code mixed setting. The code is publicly available at https://github.com/Debapriya-Tula/EACL2021-DravidianTask-Bitions.</p>
        </div>
        
      </div>
      </div>
    </li>
  </ol>
  

  <h2 class="year">2021</h2>
  <ol class="bibliography">
    <li>
      <div class="row">
  
      <div class="col-sm-2 abbr">
        <abbr class="badge">arXiv</abbr>
      </div>
  
      <div id="javed2022towards" class="col-sm-8">
            <div class="title"><a href="https://arxiv.org/abs/2107.00676" target="_blank"><b>A Primer on Pretrained Multilingual Language Models</b></a></div>
          <div class="author">
              <span style="color:cadetblue">Sumanth Doddapaneni*</span>, Gowtham Ramesh*, Anoop Kunchukuttan, Pratyush Kumar, Mitesh M Khapra
          </div>
          <div class="periodical">2021</div>
        <div class="links abbr">
          <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
        </div>
       
        <div class="abstract hidden">
          <p>Multilingual Language Models (\MLLMs) such as mBERT, XLM, XLM-R, \textit{etc.} have emerged as a viable option for bringing the power of pretraining to a large number of languages. Given their success in zero-shot transfer learning, there has emerged a large body of work in (i) building bigger \MLLMs~covering a large number of languages (ii) creating exhaustive benchmarks covering a wider variety of tasks and languages for evaluating \MLLMs~ (iii) analysing the performance of \MLLMs~on monolingual, zero-shot cross-lingual and bilingual tasks (iv) understanding the universal language patterns (if any) learnt by \MLLMs~ and (v) augmenting the (often) limited capacity of \MLLMs~ to improve their performance on seen or even unseen languages. In this survey, we review the existing literature covering the above broad areas of research pertaining to \MLLMs. Based on our survey, we recommend some promising directions of future research.</p>
        </div>
        
      </div>
      </div>
    </li>

    <li>
      <div class="row">
  
      <div class="col-sm-2 abbr">
        <abbr class="badge">EACL-W</abbr>
      </div>
  
      <div id="javed2022towards" class="col-sm-8">
            <div class="title"><a href="https://arxiv.org/abs/2107.00676" target="_blank"><b>Bitions@ DravidianLangTech-EACL2021: Ensemble of multilingual language models with pseudo labeling for offence detection in Dravidian languages</b></a></div>
          <div class="author">
              Debapriya Tula, Prathyush Potluri, Shreyas Ms, <span style="color:cadetblue">Sumanth Doddapaneni</span>, Pranjal Sahu, Rohan Sukumaran, Parth Patwa
          </div>
          <div class="periodical">2021</div>
        <div class="links abbr">
          <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a>
        </div>
       
        <div class="abstract hidden">
          <p>With the advent of social media, we have seen a proliferation of data and public discourse. Unfortunately, this includes offensive content as well. The problem is exacerbated due to the sheer number of languages spoken on these platforms and the multiple other modalities used for sharing offensive content (images, gifs, videos and more). In this paper, we propose a multilingual ensemble-based model that can identify offensive content targeted against an individual (or group) in low resource Dravidian language. Our model is able to handle code-mixed data as well as instances where the script used is mixed (for instance, Tamil and Latin). Our solution ranked number one for the Malayalam dataset and ranked 4th and 5th for Tamil and Kannada, respectively.</p>
        </div>
        
      </div>
      </div>
    </li>
  </ol>

</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2023 Sumanth Doddapaneni. All rights reserved.
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  



<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
