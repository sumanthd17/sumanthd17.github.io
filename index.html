<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Sumanth Doddapaneni</title>
  <meta name="description" content="">

  <!-- Open Graph -->


  <!-- Bootstrap & MDB -->
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg=="
    crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css"
    integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q=="
    crossorigin="anonymous" />

  <!-- Fonts & Icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"
    integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog=="
    crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css"
    integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg=="
    crossorigin="anonymous">
  <link rel="stylesheet" type="text/css"
    href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

  <!-- Code Syntax Highlighting -->
  <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

  <!-- Styles -->
  <link rel="icon" type="image/png" href="assets/imgs/iitm-logo.png">
  <link rel="stylesheet" href="/assets/css/main.css">

  <link rel="canonical" href="/">

  <!-- Theming-->



  <!-- MathJax -->
  <script defer type="text/javascript" id="MathJax-script"
    src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


</head>

<body class="fixed-top-nav ">

  <!-- Header -->

  <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
      <div class="container">


        <!-- Social Icons -->
        <div class="row ml-1 ml-sm-0">
          <span class="contact-icon text-center">
            <a href="mailto:doddapaneni.sumanth@gmail.com"><i class="fas fa-envelope"></i></a>

            <a href="https://scholar.google.com/citations?user=_kR4rGAAAAAJ" target="_blank" title="Google Scholar"><i
                class="ai ai-google-scholar"></i></a>
            <a href="https://semanticscholar.com/author/2072738714" target="_blank" title="Semantic Scholar"><i
                class="ai ai-semantic-scholar"></i></a>


            <a href="https://github.com/sumanthd17" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>

            <a href="https://twitter.com/sumanthd17" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>





          </span>

        </div>

        <!-- Navbar Toogle -->
        <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav"
          aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar top-bar"></span>
          <span class="icon-bar middle-bar"></span>
          <span class="icon-bar bottom-bar"></span>
        </button>
        <div class="collapse navbar-collapse text-right" id="navbarNav">
          <ul class="navbar-nav ml-auto flex-nowrap">
            <!-- About -->
            <li class="nav-item active">
              <a class="nav-link" href="/">
                about

                <span class="sr-only">(current)</span>

              </a>
            </li>

            <!-- Other pages -->

            <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications

              </a>
            </li>

            <li class="nav-item ">
              <a class="nav-link" href="/assets/pdf/sumanthResume.pdf">
                resume
              </a>
            </li>



          </ul>
        </div>
      </div>
    </nav>

  </header>


  <!-- Content -->

  <div class="container mt-5">
    <div class="post">

      <header class="post-header">
        <h1 class="post-title">
          <span class="font-weight-bold">Sumanth</span> Doddapaneni
        </h1>
        <h5 class="post-description">PhD Student ‚Ä¢ <a href="https://www.iitm.ac.in" target="_blank">IIT Madras</a> ‚Ä¢ <a
            href="http://ai4bharat.iitm.ac.in" target="_blank">AI4Bharat</a>
          <!-- ‚Ä¢ <a href="https://research.google/"target="_blank">Google Research</a> -->
          <!-- ‚Ä¢ <a href="https://mila.quebec/en/" target="_blank">Mila - Quebec AI Institute </a></h5> -->
      </header>

      <article>

        <div class="profile float-right">

          <!-- <img class="img-fluid z-depth-1 rounded" src="/assets/imgs/sumanth_stupid.png"> -->
          <img class="img-circle" src="/assets/imgs/sumanth_better.jpg">

          <div class="address">
            619 NAC2, IIT Madras
          </div>

        </div>


        <div class="clearfix">

          <p>Hello | ‡∞®‡∞Æ‡∞∏‡±ç‡∞ï‡∞æ‡∞∞‡∞Ç | ‡§®‡§Æ‡§∏‡•ç‡§§‡•á</p>

          <p>I am a <s>first, second</s> third year PhD Student at <a href="https://www.iitm.ac.in">IIT Madras</a>
            & <a href="http://ai4bharat.iitm.ac.in" target="_blank">AI4Bharat</a>, where I'm advised by <a
              href="http://www.cse.iitm.ac.in/~miteshk/" target="_blank">Mitesh M. Khapra</a> and <a
              href="http://anoopk.in" target="_blank">Anoop Kunchukuttan</a>. My PhD research is supported by <a
              href="https://research.google/outreach/phd-fellowship/recipients/?category=2023">Google PhD Fellowship</a>
            2023.
          </p>
          <p>
            My research interests are aligned towards Multilingual Learning for building Language Models, Machine
            Translation and Speech Recognition Models. One of the primary goals of my research is to develop models and
            data for under-resourced languages and make NLP technologies accessible to a much wider audience. Released
            <a href="https://github.com/AI4Bharat/IndicBERT">IndicBERT</a>, <a
              href="https://github.com/AI4Bharat/indicTrans">IndicTrans</a> and <a
              href="https://arxiv.org/abs/2111.03945">IndicWav2Vec</a> as part of this initiative.
          </p>
          <p>
            Previously, I interned at Google Research in Bangalore, working on improving multilingual generation,
            working with from <a href="https://research.google/people/108319/" target="_blank">Nitish Gupta</a> and <a
              href="https://research.google/people/ParthaTalukdar/" target="_blank">Partha Talukdar</a>. I also spent a
            summer at Google Research in Mountain View, focusing on language model personalization, working with <a
              href="https://www.linkedin.com/in/krishna-sayana-439a8312/" target="_blank">Krishna Sayana</a>. I
            collaborated with <a href="https://www.rahular.com/" target="_blank">Rahul Aralikatte</a> at Mila - Quebec
            AI Institute on multilingual summarization.
          </p>
          <p>If you wanna chat about research/academia/whatever, feel free to reach out to <a
              href="mailto:sumanthd@cse.iitm.ac.in">sumanthd@cse.iitm.ac.in</a></p>

        </div>


        <div class="news">
          <h2>news</h2>

          <div class="table-responsive" style="height: 300px; overflow-y: scroll;">
            <table class="table table-sm table-borderless">

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Nov, 2024</span></abbr></th>
                <td>
                  Talk at Google Research, Mountain View on "Rethinking Evaluator LLMs With a Cross-Lingual Twist".
                  Thanks Krishna Sayana for hosting me!
                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Nov, 2024</span></abbr></th>
                <td>
                  Talk at Language Technologies Institute (LTI) @ Carnegie Mellon University (CMU) on "Rethinking
                  Evaluator LLMs With a Cross-Lingual Twist". Thanks <a href="https://simran-khanuja.github.io/">Simran
                    Khanuja</a> for hosting me!
                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Nov, 2024</span></abbr></th>
                <td>
                  Attending EMNLP 2024, Miami, USA üá∫üá∏ to present <a href="http://arxiv.org/abs/2406.13439">FBI</a>
                  work! Let's catch up if you are there!
                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Oct, 2024</span></abbr></th>
                <td>
                  Talk at IT University of Copenhagen on "Rethinking Evaluator LLMs With a Cross-Lingual Twist". Thanks
                  <a href="https://ratishsp.github.io/">Ratish Puduppully</a> for hosting me!
                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Oct, 2024</span></abbr></th>
                <td>
                  Our pre-print <a href="http://arxiv.org/abs/2410.13394">CIA: Cross-Lingual Auto Evaluation for
                    Assessing Multilingual LLMs</a> is out on arxiv!
                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Sep, 2024</span></abbr></th>
                <td>
                  <a href="http://arxiv.org/abs/2406.13439">FBI</a> paper has been accepted at EMNLP 2024!
                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Sep, 2024</span></abbr></th>
                <td>
                  After almost 3 years of reviewing our survey paper on <a
                    href="https://arxiv.org/abs/2107.00676">multilingual language models</a>(pre-gpt era) paper has been
                  accepted to ACM Computing Surveys!
                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Aug, 2024</span></abbr></th>
                <td>
                  üèÜ Delighted to share that our work <a href="https://arxiv.org/abs/2403.06350">IndicLLMSuite</a> has
                  received the Outstanding Paper Award at ACL 2024!
                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Aug, 2024</span></abbr></th>
                <td>
                  Attending ACL 2024, Bangkok, Thailand üáπüá≠! Let's catch up if you are there!
                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">June, 2024</span></abbr></th>
                <td>

                  Our pre-print <a href="http://arxiv.org/abs/2406.13439">FBI: Finding Blindspots in LLM Evaluations
                    with Interpretable Checklists</a> is out on arxiv!
                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">May, 2024</span></abbr></th>
                <td>

                  <a href="https://arxiv.org/abs/2403.06350">IndicLLMSuite</a> has been accepted at ACL 2024!
                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Feb, 2024</span></abbr></th>
                <td>

                  I'll be at the Google Research Week, between Feb 1-3, 2024. Let's catch up if you are there!
                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Dec, 2023</span></abbr></th>
                <td>

                  Attending EMNLP 2023, Singapore üá∏üá¨! Let's catch up if you are there!

                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Nov, 2023</span></abbr></th>
                <td>

                  My research is now funded by <a
                    href="https://research.google/outreach/phd-fellowship/recipients/?category=2023">Google PhD
                    Fellowship</a>. Thank You, Google!


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Nov, 2023</span></abbr></th>
                <td>

                  Will start as a Research Intern at Google Research, India üáÆüá≥! Will be in Bangalore till March'24.
                  Let's catch up if you are here!


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Oct, 2023</span></abbr></th>
                <td>

                  Will be traveling along the East Coast during the last 2 weeks of October, Boston (Oct 22-24), New
                  York (Oct 24-28), Pittsburg (Oct 28-31) and back in the Bay Area till Nov 4. Come say Hi, and let's
                  walk around the streets, eat good food and maybe talk NLP!


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">July, 2023</span></abbr></th>
                <td>

                  Will be attending ACL 2023 in Toronto, Canada üá®üá¶! I will be presenting <a
                    href="https://arxiv.org/abs/2212.05409">IndicXTREME</a>, <a
                    href="https://arxiv.org/abs/2212.10168">Naamapadam</a>, and <a
                    href="https://arxiv.org/abs/2305.05858">VƒÅrta</a>. Let's catch up if you are here!


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">June, 2023</span></abbr></th>
                <td>

                  Started as a Student Researcher at Google Research, Mountain View üá∫üá∏! Will be in the Bay Area till
                  November. Let's catch up if you are here!


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">May, 2023</span></abbr></th>
                <td>

                  Released <a href="https://github.com/AI4Bharat/indicTrans2">IndicTrans2</a>, this is the first model
                  to support all 22 Scheduled Indian languages. More details in the <a
                    href="https://arxiv.org/abs/2305.16307">Paper</a>. Kudos to entire <a
                    href="https://ai4bharat.iitm.ac.in/">AI4Bharat</a> Team for pulling off this herculean effort.


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">May, 2023</span></abbr></th>
                <td>

                  Released a pre-print "A Comprehensive Analysis of Adapter Efficiency". Paper available <a
                    href="https://arxiv.org/abs/2305.07491">here</a>. Kudos to Nandini Mundra for driving this work!


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">May, 2023</span></abbr></th>
                <td>

                  Three papers accepted at ACL 2023. Pre-prints - <a
                    href="https://arxiv.org/abs/2212.05409">IndicXTREME</a>, <a
                    href="https://arxiv.org/abs/2305.05858">VƒÅrta</a>, <a
                    href="https://arxiv.org/abs/2212.10168">Naamapadam</a>


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Apr, 2023</span></abbr></th>
                <td>

                  Our paper <a href="https://dl.acm.org/doi/10.1145/3593042" target="_blank">A Survey of Adversarial
                    Defences and Robustness in NLP</a> is accepted at <a href="https://dl.acm.org/journal/csur">ACM
                    Computing Surveys</a>.


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Feb, 2023</span></abbr></th>
                <td>

                  Talk at Google Research India on Building Natural Language Understanding (NLU) capabilities for Indic
                  languages. Thanks <a href="https://research.google/people/108319/">Nitish Gupta</a> and <a
                    href="https://research.google/people/ParthaTalukdar/">Partha Talukdar</a> for hosting me!


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Feb, 2023</span></abbr></th>
                <td>

                  Our paper <a href="https://arxiv.org/abs/2208.12666" target="_blank">Effectiveness of Mining Audio and
                    Text Pairs from Public Data for Improving ASR Systems for Low-Resource Languages</a> is accepted at
                  <a href="https://2023.ieeeicassp.org">ICASSP 2023</a>.


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Jan, 2023</span></abbr></th>
                <td>

                  I'll be attending Google Research Week 2023! Let's catch up if you are there!


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Dec, 2022</span></abbr></th>
                <!--a href="#" class="btn btn-sm z-depth-0" role="button" target="_blank">Oct 6, 2022</a></th-->
                <td>

                  Released <a href="https://ai4bharat.iitm.ac.in/naamapadam">Naamapadam</a>. released. Paper is
                  available <a href="https://arxiv.org/abs/2212.10168">here</a>.

                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Dec, 2022</span></abbr></th>
                <!--a href="#" class="btn btn-sm z-depth-0" role="button" target="_blank">Oct 6, 2022</a></th-->
                <td>

                  Released <a href="https://ai4bharat.iitm.ac.in/indicxtreme">IndicXTREME</a> and <a
                    href="https://ai4bharat.iitm.ac.in/bertv2">IndicBERT v2</a>. released. Paper is available <a
                    href="https://arxiv.org/abs/2212.05409">here</a>.

                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Dec, 2022</span></abbr></th>
                <!--a href="#" class="btn btn-sm z-depth-0" role="button" target="_blank">Oct 6, 2022</a></th-->
                <td>

                  Attending EMNLP 2022, Abu Dhabi üá¶üá™! Let's catch up if you are there!

                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Nov, 2022</span></abbr></th>
                <!--a href="#" class="btn btn-sm z-depth-0" role="button" target="_blank">Oct 6, 2022</a></th-->
                <td>

                  I'll be attending ALPS 2023! Let's catch up if you are there!


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Sept, 2022</span></abbr></th>
                <!--a href="#" class="btn btn-sm z-depth-0" role="button" target="_blank">Oct 6, 2022</a></th-->
                <td>

                  Relased a pre-print of our paper <a href="https://arxiv.org/abs/2208.12666"
                    target="_blank">Effectiveness of Mining Audio and Text Pairs from Public Data for Improving ASR
                    Systems for Low-Resource Languages</a>. <b>Work led by Kaushal Bhogale </b>


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">May, 2022</span></abbr></th>
                <!--a href="#" class="btn btn-sm z-depth-0" role="button" target="_blank">Aug 22, 2022</a></th-->
                <td>

                  Presenting <a
                    href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00452/109468">Samanantar</a> at ACL
                  2022, Dublin üáÆüá™ (Thank You, Prof. Mitesh Khapra). In-person talk (25/05, session 7) & Poster (25/05,
                  session 6). Come say Hi, and let's talk NMT


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Feb, 2022</span></abbr></th>
                <!--a href="#" class="btn btn-sm z-depth-0" role="button" target="_blank">Aug 16, 2022</a></th-->
                <td>

                  Presenting <a href="https://arxiv.org/abs/2111.03945">IndicWav2Vec</a> at AAAI 2022.


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Feb, 2022</span></abbr></th>
                <!--a href="#" class="btn btn-sm z-depth-0" role="button" target="_blank">Jul 29, 2022</a></th-->
                <td>

                  I'll be attending the Google Research Week 2022! Feel free to get in touch if you are attending the
                  same.


                </td>
              </tr>

            </table>
          </div>

          <article>
            <div class="publications">
              <h2>selected papers</h2>
              <ol class="bibliography">

                <li>
                  <div class="row">

                    <div class="col-sm-2 abbr">
                      <abbr class="badge">ArXiv</abbr>
                    </div>

                    <div id="doddapaneni2024finding" class="col-sm-8">
                      <div class="title"><b>Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs</b>
                      </div>
                      <div class="author">
                        <font size="-1">
                          <span style="color:cadetblue">Sumanth
                            Doddapaneni</span>, Mohammed Safi Ur Rahman Khan, Dilip Venkatesh, Raj Dabre, Anoop
                          Kunchukuttan, Mitesh M. Khapra
                        </font>
                      </div>
                      <div class="links abbr">
                        <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a
                          href="https://arxiv.org/abs/2410.13394" target="_blank"><i class="fas fa-file"></i></a> <a
                          href="https://github.com/AI4Bharat/CIA" target="_blank"><i class="fab fa-github"></i></a>
                      </div>

                      <div class="abstract hidden">
                        <p>Evaluating machine-generated text remains a significant challenge in NLP, especially for
                          non-English languages. Current methodologies, including automated metrics, human assessments,
                          and
                          LLM-based evaluations, predominantly focus on English, revealing a significant gap in
                          multilingual
                          evaluation frameworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an
                          extensible
                          framework that includes evaluator LLMs (Hercule) and a novel test set (Recon) specifically
                          designed for multilingual evaluation. Our test set features 500 human-annotated instructions
                          spanning various task capabilities along with human judgment scores across six languages. This
                          would enable benchmarking of general-purpose multilingual LLMs and facilitate meta-evaluation
                          of
                          Evaluator LLMs. The proposed model, Hercule, is a cross-lingual evaluation model that
                          addresses
                          the scarcity of reference answers in the target language by learning to assign scores to
                          responses
                          based on easily available reference answers in English. Our experiments demonstrate that
                          Hercule
                          aligns more closely with human judgments compared to proprietary models, demonstrating the
                          effectiveness of such cross-lingual evaluation in low resource scenarios. Further, it is also
                          effective in zero-shot evaluation on unseen languages. This study is the first comprehensive
                          examination of cross-lingual evaluation using LLMs, presenting a scalable and effective
                          approach
                          for multilingual assessment. All code, datasets, and models will be publicly available to
                          enable
                          further research in this important area.
                        </p>
                      </div>
                    </div>
                </li>

                <li>
                  <div class="row">

                    <div class="col-sm-2 abbr">
                      <abbr class="badge">EMNLP'24</abbr>
                    </div>

                    <div id="doddapaneni2024finding" class="col-sm-8">
                      <div class="title"><b>Finding Blindspots in LLM Evaluations with Interpretable Checklists</b>
                      </div>
                      <div class="author">
                        <font size="-1">
                          <span style="color:cadetblue">Sumanth
                            Doddapaneni</span>, Mohammed Safi Ur Rahman Khan, Sshubam Verma, Mitesh M. Khapra
                        </font>
                      </div>
                      <div class="links abbr">
                        <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a
                          href="https://arxiv.org/abs/2406.13439" target="_blank"><i class="fas fa-file"></i></a> <a
                          href="https://github.com/AI4Bharat/FBI" target="_blank"><i class="fab fa-github"></i></a>
                      </div>

                      <div class="abstract hidden">
                        <p>Large Language Models (LLMs) are increasingly relied upon to evaluate text outputs of other
                          LLMs, thereby influencing leaderboards and development decisions. However, concerns persist
                          over the accuracy of these assessments and the potential for misleading conclusions. In this
                          work, we investigate the effectiveness of LLMs as evaluators for text generation tasks. We
                          propose FBI, a novel framework designed to examine the proficiency of Evaluator LLMs in
                          assessing four critical abilities in other LLMs: factual accuracy, instruction following,
                          coherence in long-form writing, and reasoning proficiency. By introducing targeted
                          perturbations in answers generated by LLMs, that clearly impact one of these key capabilities,
                          we test whether an Evaluator LLM can detect these quality drops. By creating a total of 2400
                          perturbed answers covering 22 perturbation categories, we conduct a comprehensive study using
                          different evaluation strategies on five prominent LLMs commonly used as evaluators in the
                          literature. Our findings reveal significant shortcomings in current Evaluator LLMs, which
                          failed to identify quality drops in over 50\% of cases on average. Single-answer and pairwise
                          evaluations demonstrated notable limitations, whereas reference-based evaluations showed
                          comparatively better performance. These results underscore the unreliable nature of current
                          Evaluator LLMs and advocate for cautious implementation in practical applications. Code and
                          data are available at this https://github.com/AI4Bharat/FBI.
                        </p>
                      </div>
                    </div>
                </li>

                <li>
                  <div class="row">

                    <div class="col-sm-2 abbr">
                      <abbr class="badge">ACL'24</abbr>
                    </div>

                    <div id="ai4bharat2023indictrans2" class="col-sm-8">
                      <div class="title"><b>IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning
                          Datasets
                          for Indian Languages</b></div>
                      <div class="author">
                        <font size="-1">
                          Mohammed Safi Ur Rahman Khan, Priyam Mehta, Ananth Sankar, Umashankar Kumaravelan, <span
                            style="color:cadetblue">Sumanth
                            Doddapaneni</span>, Suriyaprasaad G, Varun Balan G, Sparsh Jain, Anoop Kunchukuttan,
                          Pratyush
                          Kumar, Raj Dabre, Mitesh M. Khapra
                        </font>
                      </div>
                      <div class="links abbr">
                        <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a
                          href="https://arxiv.org/abs/2403.06350" target="_blank"><i class="fas fa-file"></i></a> <a
                          href="https://github.com/AI4Bharat/IndicLLMSuite" target="_blank"><i
                            class="fab fa-github"></i></a>
                        | üèÜ Outstanding Paper
                      </div>

                      <div class="abstract hidden">
                        <p>Despite the considerable advancements in English LLMs, the progress in building comparable
                          models
                          for other languages has been hindered due to the scarcity of tailored resources. Our work aims
                          to
                          bridge this divide by introducing an expansive suite of resources specifically designed for
                          the
                          development of Indic LLMs, covering 22 languages, containing a total of 251B tokens and 74.8M
                          instruction-response pairs. Recognizing the importance of both data quality and quantity, our
                          approach combines highly curated manually verified data, unverified yet valuable data, and
                          synthetic data. We build a clean, open-source pipeline for curating pre-training data from
                          diverse
                          sources, including websites, PDFs, and videos, incorporating best practices for crawling,
                          cleaning, flagging, and deduplication. For instruction-fine tuning, we amalgamate existing
                          Indic
                          datasets, translate/transliterate English datasets into Indian languages, and utilize LLaMa2
                          and
                          Mixtral models to create conversations grounded in articles from Indian Wikipedia and Wikihow.
                          Additionally, we address toxicity alignment by generating toxic prompts for multiple scenarios
                          and
                          then generate non-toxic responses by feeding these toxic prompts to an aligned LLaMa2 model.
                          We
                          hope that the datasets, tools, and resources released as a part of this work will not only
                          propel
                          the research and development of Indic LLMs but also establish an open-source blueprint for
                          extending such efforts to other languages. The data and other artifacts created as part of
                          this
                          work are released with permissive licenses.
                        </p>
                      </div>
                    </div>
                </li>

                <li>
                  <div class="row">

                    <div class="col-sm-2 abbr">
                      <abbr class="badge">TMLR</abbr>
                    </div>

                    <div id="ai4bharat2023indictrans2" class="col-sm-8">
                      <div class="title"><b>IndicTrans2: Towards High-Quality and Accessible Machine Translation Models
                          for all 22 Scheduled Indian Languages</b></div>
                      <div class="author">
                        <font size="-1">
                          AI4Bharat, Jay Gala, Pranjal A. Chitale, Raghavan AK, <span style="color:cadetblue">Sumanth
                            Doddapaneni</span>, Varun Gumma, Aswanth Kumar, Janki Nawale, Anupama Sujatha, Ratish
                          Puduppully, Vivek Raghavan, Pratyush Kumar, Mitesh M. Khapra, Raj Dabre, Anoop Kunchukuttan
                        </font>
                      </div>
                      <div class="links abbr">
                        <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a
                          href="https://arxiv.org/abs/2305.16307" target="_blank"><i class="fas fa-file"></i></a> <a
                          href="https://github.com/ai4bharat/IndicTrans2" target="_blank"><i
                            class="fab fa-github"></i></a>
                      </div>

                      <div class="abstract hidden">
                        <p>India has a rich linguistic landscape with languages from 4 major language families spoken by
                          over a billion people. 22 of these languages are listed in the Constitution of India (referred
                          to as scheduled languages) are the focus of this work. Given the linguistic diversity,
                          high-quality and accessible Machine Translation (MT) systems are essential in a country like
                          India. Prior to this work, there was (i) no parallel training data spanning all the 22
                          languages, (ii) no robust benchmarks covering all these languages and containing content
                          relevant to India, and (iii) no existing translation models which support all the 22 scheduled
                          languages of India. In this work, we aim to address this gap by focusing on the missing pieces
                          required for enabling wide, easy, and open access to good machine translation systems for all
                          22 scheduled Indian languages. We identify four key areas of improvement: curating and
                          creating larger training datasets, creating diverse and high-quality benchmarks, training
                          multilingual models, and releasing models with open access. Our first contribution is the
                          release of the Bharat Parallel Corpus Collection (BPCC), the largest publicly available
                          parallel corpora for Indic languages. BPCC contains a total of 230M bitext pairs, of which a
                          total of 126M were newly added, including 644K manually translated sentence pairs created as
                          part of this work. Our second contribution is the release of the first n-way parallel
                          benchmark covering all 22 Indian languages, featuring diverse domains, Indian-origin content,
                          and source-original test sets. Next, we present IndicTrans2, the first model to support all 22
                          languages, surpassing existing models on multiple existing and new benchmarks created as a
                          part of this work. Lastly, to promote accessibility and collaboration, we release our models
                          and associated data with permissive licenses at https://github.com/ai4bharat/IndicTrans2.
                        </p>
                      </div>
                    </div>
                </li>

                <li>
                  <div class="row">

                    <div class="col-sm-2 abbr">
                      <abbr class="badge">ACL'23</abbr>
                    </div>

                    <div id="doddapanei2022indicxtreme" class="col-sm-8">
                      <div class="title"><b>Towards Leaving No Indic Language Behind: Building Monolingual Corpora,
                          Benchmark and Models for Indic Languages</b></div>
                      <div class="author">
                        <font size="-1">
                          <span style="color:cadetblue">Sumanth Doddapaneni</span>, Rahul Aralikatte, Gowtham Ramesh,
                          Shreya Goyal, Mitesh M. Khapra, Anoop Kunchukuttan, Pratyush Kumar
                        </font>
                      </div>
                      <div class="links abbr">
                        <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a
                          href="https://arxiv.org/abs/2212.05409" target="_blank"><i class="fas fa-file"></i></a> <a
                          href="https://github.com/AI4Bharat/IndicBERT" target="_blank"><i
                            class="fab fa-github"></i></a>
                      </div>

                      <div class="abstract hidden">
                        <p>Building Natural Language Understanding (NLU) capabilities for Indic languages, which have a
                          collective speaker base of more than one billion speakers is absolutely crucial. In this work,
                          we aim to improve the NLU capabilities of Indic languages by making contributions along 3
                          important axes (i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing on
                          Indic languages. Specifically, we curate the largest monolingual corpora, IndicCorp, with
                          20.9B tokens covering 24 languages from 4 language families - a 2.3x increase over prior work,
                          while supporting 12 additional languages. Next, we create a human-supervised benchmark,
                          IndicXTREME, consisting of nine diverse NLU tasks covering 20 languages. Across languages and
                          tasks, IndicXTREME contains a total of 105 evaluation sets, of which 52 are new contributions
                          to the literature. To the best of our knowledge, this is the first effort towards creating a
                          standard benchmark for Indic languages that aims to test the multilingual zero-shot
                          capabilities of pretrained language models. Finally, we train IndicBERT v2, a state-of-the-art
                          model supporting all the languages. Averaged across languages and tasks, the model achieves an
                          absolute improvement of 2 points over a strong baseline. The data and models are available at
                          \url{https://github.com/AI4Bharat/IndicBERT}.</p>
                      </div>

                    </div>
                  </div>
                </li>

                <li>
                  <div class="row">

                    <div class="col-sm-2 abbr">
                      <abbr class="badge">TACL</abbr>
                    </div>

                    <div id="ramesh2021samanantar" class="col-sm-8">
                      <div class="title"><b>Samanantar: The Largest Publicly Available Parallel Corpora Collection For
                          11 Indic Languages</b></div>
                      <div class="author">
                        <font size="-1">
                          Gowtham Ramesh*, <span style="color:cadetblue">Sumanth Doddapaneni*</span>, Aravinth
                          Bheemaraj, Mayank Jobanputra, Raghavan AK, Ajitesh Sharma, Sujit Sahoo, Harshita Diddee,
                          Divyanshu Kakwani, Navneet Kumar, Aswin Pradeep, Srihari Nagaraj, Kumar Deepak, Vivek
                          Raghavan, Anoop Kunchukuttan, Pratyush Kumar, Mitesh Shantadevi Khapra
                        </font>
                      </div>
                      <div class="links abbr">
                        <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a
                          href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00452/109468/Samanantar-The-Largest-Publicly-Available-Parallel"
                          target="_blank"><i class="fas fa-file"></i></a> <a
                          href="https://github.com/AI4Bharat/indicTrans" target="_blank"><i
                            class="fab fa-github"></i></a>
                      </div>

                      <div class="abstract hidden">
                        <p>We present Samanantar, the largest publicly available parallel corpora collection for Indic
                          languages. The collection contains a total of 49.7 million sentence pairs between English and
                          11 Indic languages (from two language families). Specifically, we compile 12.4 million
                          sentence pairs from existing, publicly available parallel corpora, and additionally mine 37.4
                          million sentence pairs from the Web, resulting in a 4√ó increase. We mine the parallel
                          sentences from the Web by combining many corpora, tools, and methods: (a) Web-crawled
                          monolingual corpora, (b) document OCR for extracting sentences from scanned documents, (c)
                          multilingual representation models for aligning sentences, and (d) approximate nearest
                          neighbor search for searching in a large collection of sentences. Human evaluation of samples
                          from the newly mined corpora validate the high quality of the parallel sentences across 11
                          languages. Further, we extract 83.4 million sentence pairs between all 55 Indic language pairs
                          from the English-centric parallel corpus using English as the pivot language. We trained
                          multilingual NMT models spanning all these languages on Samanantar which outperform existing
                          models and baselines on publicly available benchmarks, such as FLORES, establishing the
                          utility of Samanantar. Our data and models are available publicly at Samanantar and we hope
                          they will help advance research in NMT and multilingual NLP for Indic languages.</p>
                      </div>

                    </div>
                  </div>
                </li>

                <li>
                  <div class="row">

                    <div class="col-sm-2 abbr">
                      <abbr class="badge">AAAI'22</abbr>
                    </div>

                    <div id="javed2022towards" class="col-sm-8">
                      <div class="title"><b>Towards Building ASR Systems For The Next Billion Users</b></div>
                      <div class="author">
                        <font size="-1">
                          Tahir Javed,
                          <span style="color:cadetblue">Sumanth Doddapaneni</span>,
                          Abhigyan Raman,
                          Kaushal Santosh Bhogale,
                          Gowtham Ramesh,
                          Anoop Kunchukuttan,
                          Pratyush Kumar,
                          Mitesh M. Khapra
                        </font>
                      </div>
                      <div class="links abbr">
                        <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a
                          href="https://ojs.aaai.org/index.php/AAAI/article/view/21327" target="_blank"><i
                            class="fas fa-file"></i></a> <a href="https://github.com/AI4Bharat/indicwav2vec"
                          target="_blank"><i class="fab fa-github"></i></a>
                      </div>

                      <div class="abstract hidden">
                        <p>Recent methods in speech and language technology pretrain very large models which are
                          fine-tuned for specific tasks. However, the benefits of such large models are often limited to
                          a few resource rich languages of the world. In this work, we make multiple contributions
                          towards building ASR systems for low resource languages from the Indian subcontinent. First,
                          we curate 17,000 hours of raw speech data for 40 Indian languages from a wide variety of
                          domains including education, news, technology, and finance. Second, using this raw speech data
                          we pretrain several variants of wav2vec style models for 40 Indian languages. Third, we
                          analyze the pretrained models to find key features: codebook vectors of similar sounding
                          phonemes are shared across languages, representations across layers are discriminative of the
                          language family, and attention heads often pay attention within small local windows. Fourth,
                          we fine-tune this model for downstream ASR for 9 languages and obtain state-of-the-art results
                          on 3 public datasets, including on very low-resource languages such as Sinhala and Nepali. Our
                          work establishes that multilingual pretraining is an effective strategy for building ASR
                          systems for the linguistically diverse speakers of the Indian subcontinent.</p>
                      </div>

                    </div>
                  </div>
                </li>
              </ol>

            </div>
          </article>

          <article>
            <div class="flags">
              <h2>flags</h2>
              Conferences and internships took me here üáÆüá™ üá¶üá™ üá∫üá∏ üá®üá¶ üá∏üá¨ üáπüá≠ (so far)!
            </div>
          </article>
        </div>


      </article>

    </div>

  </div>

  <!-- Footer -->


  <footer class="fixed-bottom">
    <div class="container mt-0">
      &copy; Copyright 2024 Sumanth Doddapaneni. All rights reserved.
    </div>
  </footer>



</body>

<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"
  integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg=="
  crossorigin="anonymous"></script>

<!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js"
  integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A=="
  crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"
  integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ=="
  crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js"
  integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw=="
  crossorigin="anonymous"></script>


<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>






<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>