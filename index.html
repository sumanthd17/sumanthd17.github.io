<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Sumanth Doddapaneni</title>
  <meta name="description" content="">

  <link rel="icon" href="http://example.com/favicon.png">

  <!-- Open Graph -->


  <!-- Bootstrap & MDB -->
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg=="
    crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css"
    integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q=="
    crossorigin="anonymous" />

  <!-- Fonts & Icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"
    integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog=="
    crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css"
    integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg=="
    crossorigin="anonymous">
  <link rel="stylesheet" type="text/css"
    href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

  <!-- Code Syntax Highlighting -->
  <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

  <!-- Styles -->
  <!-- <link rel="shortcut icon" href="/assets/img/favicon.ico"> -->
  <link rel="shortcut icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>‚öõÔ∏è</text></svg>">
  <link rel="stylesheet" href="/assets/css/main.css">

  <link rel="canonical" href="/">

  <!-- Theming-->



  <!-- MathJax -->
  <script defer type="text/javascript" id="MathJax-script"
    src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


</head>

<body class="fixed-top-nav ">

  <!-- Header -->

  <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
      <div class="container">


        <!-- Social Icons -->
        <div class="row ml-1 ml-sm-0">
          <span class="contact-icon text-center">
            <a href="mailto:doddapaneni.sumanth@gmail.com"><i class="fas fa-envelope"></i></a>

            <a href="https://scholar.google.com/citations?user=_kR4rGAAAAAJ" target="_blank" title="Google Scholar"><i
                class="ai ai-google-scholar"></i></a>
            <a href="https://semanticscholar.com/author/2072738714" target="_blank" title="Semantic Scholar"><i
                class="ai ai-semantic-scholar"></i></a>


            <a href="https://github.com/sumanthd17" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>

            <a href="https://twitter.com/sumanthd17" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>





          </span>

        </div>

        <!-- Navbar Toogle -->
        <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav"
          aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar top-bar"></span>
          <span class="icon-bar middle-bar"></span>
          <span class="icon-bar bottom-bar"></span>
        </button>
        <div class="collapse navbar-collapse text-right" id="navbarNav">
          <ul class="navbar-nav ml-auto flex-nowrap">
            <!-- About -->
            <li class="nav-item active">
              <a class="nav-link" href="/">
                about

                <span class="sr-only">(current)</span>

              </a>
            </li>

            <!-- Other pages -->

            <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications

              </a>
            </li>

            <li class="nav-item ">
              <a class="nav-link" href="/assets/pdf/sumanthResume.pdf">
                resume
              </a>
            </li>



          </ul>
        </div>
      </div>
    </nav>

  </header>


  <!-- Content -->

  <div class="container mt-5">
    <div class="post">

      <header class="post-header">
        <h1 class="post-title">
          <span class="font-weight-bold">Sumanth</span> Doddapaneni
        </h1>
        <h5 class="post-description">PhD Student ‚Ä¢ <a href="https://www.iitm.ac.in" target="_blank">IIT Madras</a> ‚Ä¢ <a
            href="http://ai4bharat.iitm.ac.in" target="_blank">AI4Bharat</a> ‚Ä¢ <a href="https://research.google/"
            target="_blank">Google Research</a>
          <!-- ‚Ä¢ <a href="https://mila.quebec/en/" target="_blank">Mila - Quebec AI Institute </a></h5> -->
      </header>

      <article>

        <div class="profile float-right">

          <!-- <img class="img-fluid z-depth-1 rounded" src="/assets/imgs/sumanth_stupid.png"> -->
          <img class="img-circle" src="/assets/imgs/sumanth_better.jpg">

          <div class="address">
            Research Intern, Google Research, Bangalore
          </div>

        </div>


        <div class="clearfix">

          <p>Hello | ‡∞®‡∞Æ‡∞∏‡±ç‡∞ï‡∞æ‡∞∞‡∞Ç | ‡§®‡§Æ‡§∏‡•ç‡§§‡•á</p>

          <p>I am a <s>first</s> <s>second</s>third year PhD Student at <a href="https://www.iitm.ac.in">IIT Madras</a>
            & <a href="http://ai4bharat.iitm.ac.in" target="_blank">AI4Bharat</a>, where I'm advised by <a
              href="http://www.cse.iitm.ac.in/~miteshk/" target="_blank">Mitesh M. Khapra</a> and <a
              href="http://anoopk.in" target="_blank">Anoop Kunchukuttan</a>. My PhD research is supported by <a
              href="https://research.google/outreach/phd-fellowship/recipients/?category=2023">Google PhD Fellowship</a>
            2023.
          </p>
          <p>
            My research interests are aligned towards Multilingual Learning for building Language Models, Machine
            Translation and Speech Recognition Models. One of the primary goals of my research is to develop models and
            data for under-resourced languages and make NLP technologies accessible to a much wider audience. Released
            <a href="https://github.com/AI4Bharat/IndicBERT">IndicBERT</a>, <a
              href="https://github.com/AI4Bharat/indicTrans">IndicTrans</a> and <a
              href="https://arxiv.org/abs/2111.03945">IndicWav2Vec</a> as part of this initiative. I also collaborated
            with <a href="http://www.rahular.com">Rahul Aralikatte </a> at <a href="https://mila.quebec/en/"
              target="_blank">Mila - Quebec AI Institute</a>, working on multilingual summarization.
          </p>
          <p>
            Feel free to check out my <a href="/assets/pdf/sumanthResume.pdf" target="_blank">resume</a> and drop me an
            <a href="mailto:doddapaneni.sumanth@gmail.com" target="_blank"> email </a> if you want to chat with me!
          </p>

        </div>


        <div class="news">
          <h2>news</h2>

          <div class="table-responsive" style="height: 300px; overflow-y: scroll;">
            <table class="table table-sm table-borderless">

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Dec, 2022</span></abbr></th>
                <!--a href="#" class="btn btn-sm z-depth-0" role="button" target="_blank">Oct 6, 2022</a></th-->
                <td>

                  Attending EMNLP 2023, Singapore üá∏üá¨! Let's catch up if you are there!

                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Nov, 2023</span></abbr></th>
                <td>

                  My research is now funded by <a
                    href="https://research.google/outreach/phd-fellowship/recipients/?category=2023">Google PhD
                    Fellowship</a>. Thank You, Google!


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Nov, 2023</span></abbr></th>
                <td>

                  Will start as a Research Intern at Google Research, India üáÆüá≥! Will be in Bangalore till March'24.
                  Let's catch up if you are here!


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Oct, 2023</span></abbr></th>
                <td>

                  Will be traveling along the East Coast during the last 2 weeks of October, Boston (Oct 22-24), New
                  York (Oct 24-28), Pittsburg (Oct 28-31) and back in the Bay Area till Nov 4. Come say Hi, and let's
                  walk around the streets, eat good food and maybe talk NLP!


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">July, 2023</span></abbr></th>
                <td>

                  Will be attending ACL 2023 in Toronto, Canada üá®üá¶! I will be presenting <a
                    href="https://arxiv.org/abs/2212.05409">IndicXTREME</a>, <a
                    href="https://arxiv.org/abs/2212.10168">Naamapadam</a>, and <a
                    href="https://arxiv.org/abs/2305.05858">VƒÅrta</a>. Let's catch up if you are here!


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">June, 2023</span></abbr></th>
                <td>

                  Started as a Student Researcher at Google Research, Mountain View üá∫üá∏! Will be in the Bay Area till
                  November. Let's catch up if you are here!


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">May, 2023</span></abbr></th>
                <td>

                  Released <a href="https://github.com/AI4Bharat/indicTrans2">IndicTrans2</a>, this is the first model
                  to support all 22 Scheduled Indian languages. More details in the <a
                    href="https://arxiv.org/abs/2305.16307">Paper</a>. Kudos to entire <a
                    href="https://ai4bharat.iitm.ac.in/">AI4Bharat</a> Team for pulling off this herculean effort.


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">May, 2023</span></abbr></th>
                <td>

                  Released a pre-print "A Comprehensive Analysis of Adapter Efficiency". Paper available <a
                    href="https://arxiv.org/abs/2305.07491">here</a>. Kudos to Nandini Mundra for driving this work!


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">May, 2023</span></abbr></th>
                <td>

                  Three papers accepted at ACL 2023. Pre-prints - <a
                    href="https://arxiv.org/abs/2212.05409">IndicXTREME</a>, <a
                    href="https://arxiv.org/abs/2305.05858">VƒÅrta</a>, <a
                    href="https://arxiv.org/abs/2212.10168">Naamapadam</a>


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Apr, 2023</span></abbr></th>
                <td>

                  Our paper <a href="https://dl.acm.org/doi/10.1145/3593042" target="_blank">A Survey of Adversarial
                    Defences and Robustness in NLP</a> is accepted at <a href="https://dl.acm.org/journal/csur">ACM
                    Computing Surveys</a>.


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Feb, 2023</span></abbr></th>
                <td>

                  Talk at Google Research India on Building Natural Language Understanding (NLU) capabilities for Indic
                  languages. Thanks <a href="https://research.google/people/108319/">Nitish Gupta</a> and <a
                    href="https://research.google/people/ParthaTalukdar/">Partha Talukdar</a> for hosting me!


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Feb, 2023</span></abbr></th>
                <td>

                  Our paper <a href="https://arxiv.org/abs/2208.12666" target="_blank">Effectiveness of Mining Audio and
                    Text Pairs from Public Data for Improving ASR Systems for Low-Resource Languages</a> is accepted at
                  <a href="https://2023.ieeeicassp.org">ICASSP 2023</a>.


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Jan, 2023</span></abbr></th>
                <td>

                  I'll be attending Google Research Week 2023! Let's catch up if you are there!


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Dec, 2022</span></abbr></th>
                <!--a href="#" class="btn btn-sm z-depth-0" role="button" target="_blank">Oct 6, 2022</a></th-->
                <td>

                  Released <a href="https://ai4bharat.iitm.ac.in/naamapadam">Naamapadam</a>. released. Paper is
                  available <a href="https://arxiv.org/abs/2212.10168">here</a>.

                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Dec, 2022</span></abbr></th>
                <!--a href="#" class="btn btn-sm z-depth-0" role="button" target="_blank">Oct 6, 2022</a></th-->
                <td>

                  Released <a href="https://ai4bharat.iitm.ac.in/indicxtreme">IndicXTREME</a> and <a
                    href="https://ai4bharat.iitm.ac.in/bertv2">IndicBERT v2</a>. released. Paper is available <a
                    href="https://arxiv.org/abs/2212.05409">here</a>.

                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Dec, 2022</span></abbr></th>
                <!--a href="#" class="btn btn-sm z-depth-0" role="button" target="_blank">Oct 6, 2022</a></th-->
                <td>

                  Attending EMNLP 2022, Abu Dhabi üá¶üá™! Let's catch up if you are there!

                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Nov, 2022</span></abbr></th>
                <!--a href="#" class="btn btn-sm z-depth-0" role="button" target="_blank">Oct 6, 2022</a></th-->
                <td>

                  I'll be attending ALPS 2023! Let's catch up if you are there!


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Sept, 2022</span></abbr></th>
                <!--a href="#" class="btn btn-sm z-depth-0" role="button" target="_blank">Oct 6, 2022</a></th-->
                <td>

                  Relased a pre-print of our paper <a href="https://arxiv.org/abs/2208.12666"
                    target="_blank">Effectiveness of Mining Audio and Text Pairs from Public Data for Improving ASR
                    Systems for Low-Resource Languages</a>. <b>Work led by Kaushal Bhogale </b>


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">May, 2022</span></abbr></th>
                <!--a href="#" class="btn btn-sm z-depth-0" role="button" target="_blank">Aug 22, 2022</a></th-->
                <td>

                  Presenting <a
                    href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00452/109468">Samanantar</a> at ACL
                  2022, Dublin üáÆüá™ (Thank You, Prof. Mitesh Khapra). In-person talk (25/05, session 7) & Poster (25/05,
                  session 6). Come say Hi, and let's talk NMT


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Feb, 2022</span></abbr></th>
                <!--a href="#" class="btn btn-sm z-depth-0" role="button" target="_blank">Aug 16, 2022</a></th-->
                <td>

                  Presenting <a href="https://arxiv.org/abs/2111.03945">IndicWav2Vec</a> at AAAI 2022.


                </td>
              </tr>

              <tr>
                <th scope="row abbr"> <abbr class="badge"><span style="color:#000000">Feb, 2022</span></abbr></th>
                <!--a href="#" class="btn btn-sm z-depth-0" role="button" target="_blank">Jul 29, 2022</a></th-->
                <td>

                  I'll be attending the Google Research Week 2022! Feel free to get in touch if you are attending the
                  same.


                </td>
              </tr>

            </table>
          </div>

          <article>
            <div class="publications">
              <h2>selected papers</h2>
              <ol class="bibliography">

                <li>
                  <div class="row">

                    <div class="col-sm-2 abbr">
                      <abbr class="badge">ArXiv 2023</abbr>
                    </div>

                    <div id="ai4bharat2023indictrans2" class="col-sm-8">
                      <div class="title"><b>IndicTrans2: Towards High-Quality and Accessible Machine Translation Models
                          for all 22 Scheduled Indian Languages</b></div>
                      <div class="author">
                        <font size="-1">
                          AI4Bharat, Jay Gala, Pranjal A. Chitale, Raghavan AK, <span style="color:cadetblue">Sumanth
                            Doddapaneni</span>, Varun Gumma, Aswanth Kumar, Janki Nawale, Anupama Sujatha, Ratish
                          Puduppully, Vivek Raghavan, Pratyush Kumar, Mitesh M. Khapra, Raj Dabre, Anoop Kunchukuttan
                        </font>
                      </div>
                      <div class="links abbr">
                        <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a
                          href="https://arxiv.org/abs/2305.16307" target="_blank"><i class="fas fa-file"></i></a> <a
                          href="https://github.com/ai4bharat/IndicTrans2" target="_blank"><i
                            class="fab fa-github"></i></a>
                      </div>

                      <div class="abstract hidden">
                        <p>India has a rich linguistic landscape with languages from 4 major language families spoken by
                          over a billion people. 22 of these languages are listed in the Constitution of India (referred
                          to as scheduled languages) are the focus of this work. Given the linguistic diversity,
                          high-quality and accessible Machine Translation (MT) systems are essential in a country like
                          India. Prior to this work, there was (i) no parallel training data spanning all the 22
                          languages, (ii) no robust benchmarks covering all these languages and containing content
                          relevant to India, and (iii) no existing translation models which support all the 22 scheduled
                          languages of India. In this work, we aim to address this gap by focusing on the missing pieces
                          required for enabling wide, easy, and open access to good machine translation systems for all
                          22 scheduled Indian languages. We identify four key areas of improvement: curating and
                          creating larger training datasets, creating diverse and high-quality benchmarks, training
                          multilingual models, and releasing models with open access. Our first contribution is the
                          release of the Bharat Parallel Corpus Collection (BPCC), the largest publicly available
                          parallel corpora for Indic languages. BPCC contains a total of 230M bitext pairs, of which a
                          total of 126M were newly added, including 644K manually translated sentence pairs created as
                          part of this work. Our second contribution is the release of the first n-way parallel
                          benchmark covering all 22 Indian languages, featuring diverse domains, Indian-origin content,
                          and source-original test sets. Next, we present IndicTrans2, the first model to support all 22
                          languages, surpassing existing models on multiple existing and new benchmarks created as a
                          part of this work. Lastly, to promote accessibility and collaboration, we release our models
                          and associated data with permissive licenses at https://github.com/ai4bharat/IndicTrans2.
                        </p>
                      </div>
                    </div>
                </li>


                <li>
                  <div class="row">

                    <div class="col-sm-2 abbr">
                      <abbr class="badge">ACL 2023</abbr>
                    </div>

                    <div id="aralikatte2023varta" class="col-sm-8">
                      <div class="title"><b>VƒÅrta: A Large-Scale Headline-Generation Dataset for Indic Languages</b>
                      </div>
                      <div class="author">
                        <font size="-1">
                          Rahul Aralikatte, Ziling Cheng, <span style="color:cadetblue">Sumanth Doddapaneni</span>,
                          Jackie Chi Kit Cheung
                        </font>
                      </div>
                      <div class="links abbr">
                        <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a
                          href="https://arxiv.org/abs/2305.05858" target="_blank"><i class="fas fa-file"></i></a> <a
                          href="https://github.com/rahular/varta" target="_blank"><i class="fab fa-github"></i></a>
                      </div>

                      <div class="abstract hidden">
                        <p>We present VƒÅrta, a large-scale multilingual dataset for headline generation in Indic
                          languages. This dataset includes 41.8 million news articles in 14 different Indic languages
                          (and English), which come from a variety of high-quality sources. To the best of our
                          knowledge, this is the largest collection of curated articles for Indic languages currently
                          available. We use the data collected in a series of experiments to answer important questions
                          related to Indic NLP and multilinguality research in general. We show that the dataset is
                          challenging even for state-of-the-art abstractive models and that they perform only slightly
                          better than extractive baselines. Owing to its size, we also show that the dataset can be used
                          to pretrain strong language models that outperform competitive baselines in both NLU and NLG
                          benchmarks.</p>
                      </div>
                    </div>
                </li>

                <li>
                  <div class="row">

                    <div class="col-sm-2 abbr">
                      <abbr class="badge">ACL 2023</abbr>
                    </div>
                    <div id="mahaske2022naamapadam" class="col-sm-8">
                      <div class="title"><b>Naamapadam: A Large-Scale Named Entity Annotated Data for Indic
                          Languages</b></div>
                      <div class="author">
                        <font size="-1">
                          Arnav Mhaske, Harshit Kedia, <span style="color:cadetblue">Sumanth Doddapaneni</span>, Mitesh
                          M. Khapra, Pratyush Kumar, Rudra Murthy V, Anoop Kunchukuttan
                        </font>
                      </div>
                      <div class="links abbr">
                        <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a
                          href="https://arxiv.org/abs/2212.10168" target="_blank"><i class="fas fa-file"></i></a>
                      </div>

                      <div class="abstract hidden">
                        <p>We present, Naamapadam, the largest publicly available Named Entity Recognition (NER) dataset
                          for the 11 major Indian languages from two language families. In each language, it contains
                          more than 400k sentences annotated with a total of at least 100k entities from three standard
                          entity categories (Person, Location and Organization) for 9 out of the 11 languages. The
                          training dataset has been automatically created from the Samanantar parallel corpus by
                          projecting automatically tagged entities from an English sentence to the corresponding Indian
                          language sentence. We also create manually annotated testsets for 8 languages containing
                          approximately 1000 sentences per language. We demonstrate the utility of the obtained dataset
                          on existing testsets and the Naamapadam-test data for 8 Indic languages. We also release
                          IndicNER, a multilingual mBERT model fine-tuned on the Naamapadam training set. IndicNER
                          achieves the best F1 on the Naamapadam-test set compared to an mBERT model fine-tuned on
                          existing datasets. IndicNER achieves an F1 score of more than 80 for 7 out of 11 Indic
                          languages. The dataset and models are available under open-source licenses at
                          https://ai4bharat.iitm.ac.in/naamapadam.</p>
                      </div>

                    </div>
                  </div>
                </li>


                <li>
                  <div class="row">

                    <div class="col-sm-2 abbr">
                      <abbr class="badge">ACL 2023</abbr>
                    </div>

                    <div id="doddapanei2022indicxtreme" class="col-sm-8">
                      <div class="title"><b>Towards Leaving No Indic Language Behind: Building Monolingual Corpora,
                          Benchmark and Models for Indic Languages</b></div>
                      <div class="author">
                        <font size="-1">
                          <span style="color:cadetblue">Sumanth Doddapaneni</span>, Rahul Aralikatte, Gowtham Ramesh,
                          Shreya Goyal, Mitesh M. Khapra, Anoop Kunchukuttan, Pratyush Kumar
                        </font>
                      </div>
                      <div class="links abbr">
                        <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a
                          href="https://arxiv.org/abs/2212.05409" target="_blank"><i class="fas fa-file"></i></a> <a
                          href="https://github.com/AI4Bharat/IndicBERT" target="_blank"><i
                            class="fab fa-github"></i></a>
                      </div>

                      <div class="abstract hidden">
                        <p>Building Natural Language Understanding (NLU) capabilities for Indic languages, which have a
                          collective speaker base of more than one billion speakers is absolutely crucial. In this work,
                          we aim to improve the NLU capabilities of Indic languages by making contributions along 3
                          important axes (i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing on
                          Indic languages. Specifically, we curate the largest monolingual corpora, IndicCorp, with
                          20.9B tokens covering 24 languages from 4 language families - a 2.3x increase over prior work,
                          while supporting 12 additional languages. Next, we create a human-supervised benchmark,
                          IndicXTREME, consisting of nine diverse NLU tasks covering 20 languages. Across languages and
                          tasks, IndicXTREME contains a total of 105 evaluation sets, of which 52 are new contributions
                          to the literature. To the best of our knowledge, this is the first effort towards creating a
                          standard benchmark for Indic languages that aims to test the multilingual zero-shot
                          capabilities of pretrained language models. Finally, we train IndicBERT v2, a state-of-the-art
                          model supporting all the languages. Averaged across languages and tasks, the model achieves an
                          absolute improvement of 2 points over a strong baseline. The data and models are available at
                          \url{https://github.com/AI4Bharat/IndicBERT}.</p>
                      </div>

                    </div>
                  </div>
                </li>

                <li>
                  <div class="row">

                    <div class="col-sm-2 abbr">
                      <abbr class="badge">TACL</abbr>
                    </div>

                    <div id="ramesh2021samanantar" class="col-sm-8">
                      <div class="title"><b>Samanantar: The Largest Publicly Available Parallel Corpora Collection For
                          11 Indic Languages</b></div>
                      <div class="author">
                        <font size="-1">
                          Gowtham Ramesh*, <span style="color:cadetblue">Sumanth Doddapaneni*</span>, Aravinth
                          Bheemaraj, Mayank Jobanputra, Raghavan AK, Ajitesh Sharma, Sujit Sahoo, Harshita Diddee,
                          Divyanshu Kakwani, Navneet Kumar, Aswin Pradeep, Srihari Nagaraj, Kumar Deepak, Vivek
                          Raghavan, Anoop Kunchukuttan, Pratyush Kumar, Mitesh Shantadevi Khapra
                        </font>
                      </div>
                      <div class="links abbr">
                        <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a
                          href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00452/109468/Samanantar-The-Largest-Publicly-Available-Parallel"
                          target="_blank"><i class="fas fa-file"></i></a> <a
                          href="https://github.com/AI4Bharat/indicTrans" target="_blank"><i
                            class="fab fa-github"></i></a>
                      </div>

                      <div class="abstract hidden">
                        <p>We present Samanantar, the largest publicly available parallel corpora collection for Indic
                          languages. The collection contains a total of 49.7 million sentence pairs between English and
                          11 Indic languages (from two language families). Specifically, we compile 12.4 million
                          sentence pairs from existing, publicly available parallel corpora, and additionally mine 37.4
                          million sentence pairs from the Web, resulting in a 4√ó increase. We mine the parallel
                          sentences from the Web by combining many corpora, tools, and methods: (a) Web-crawled
                          monolingual corpora, (b) document OCR for extracting sentences from scanned documents, (c)
                          multilingual representation models for aligning sentences, and (d) approximate nearest
                          neighbor search for searching in a large collection of sentences. Human evaluation of samples
                          from the newly mined corpora validate the high quality of the parallel sentences across 11
                          languages. Further, we extract 83.4 million sentence pairs between all 55 Indic language pairs
                          from the English-centric parallel corpus using English as the pivot language. We trained
                          multilingual NMT models spanning all these languages on Samanantar which outperform existing
                          models and baselines on publicly available benchmarks, such as FLORES, establishing the
                          utility of Samanantar. Our data and models are available publicly at Samanantar and we hope
                          they will help advance research in NMT and multilingual NLP for Indic languages.</p>
                      </div>

                    </div>
                  </div>
                </li>

                <li>
                  <div class="row">

                    <div class="col-sm-2 abbr">
                      <abbr class="badge">AAAI</abbr>
                    </div>

                    <div id="javed2022towards" class="col-sm-8">
                      <div class="title"><b>Towards Building ASR Systems For The Next Billion Users</b></div>
                      <div class="author">
                        <font size="-1">
                          Tahir Javed,
                          <span style="color:cadetblue">Sumanth Doddapaneni</span>,
                          Abhigyan Raman,
                          Kaushal Santosh Bhogale,
                          Gowtham Ramesh,
                          Anoop Kunchukuttan,
                          Pratyush Kumar,
                          Mitesh M. Khapra
                        </font>
                      </div>
                      <div class="links abbr">
                        <a class="abstract badge" role="button"><span style="color:cadetblue">Abstract</span></a> <a
                          href="https://ojs.aaai.org/index.php/AAAI/article/view/21327" target="_blank"><i
                            class="fas fa-file"></i></a> <a href="https://github.com/AI4Bharat/indicwav2vec"
                          target="_blank"><i class="fab fa-github"></i></a>
                      </div>

                      <div class="abstract hidden">
                        <p>Recent methods in speech and language technology pretrain very large models which are
                          fine-tuned for specific tasks. However, the benefits of such large models are often limited to
                          a few resource rich languages of the world. In this work, we make multiple contributions
                          towards building ASR systems for low resource languages from the Indian subcontinent. First,
                          we curate 17,000 hours of raw speech data for 40 Indian languages from a wide variety of
                          domains including education, news, technology, and finance. Second, using this raw speech data
                          we pretrain several variants of wav2vec style models for 40 Indian languages. Third, we
                          analyze the pretrained models to find key features: codebook vectors of similar sounding
                          phonemes are shared across languages, representations across layers are discriminative of the
                          language family, and attention heads often pay attention within small local windows. Fourth,
                          we fine-tune this model for downstream ASR for 9 languages and obtain state-of-the-art results
                          on 3 public datasets, including on very low-resource languages such as Sinhala and Nepali. Our
                          work establishes that multilingual pretraining is an effective strategy for building ASR
                          systems for the linguistically diverse speakers of the Indian subcontinent.</p>
                      </div>

                    </div>
                  </div>
                </li>
              </ol>

            </div>
          </article>

          <!-- <h5>üï∞Ô∏è  <a href="/oldnews/">older...</a></h5> -->
          <article>
            <div class="flags">
              <h2>flags</h2>
              Conferences and internships took me here! Flags of my adventures: üáÆüá™ üá¶üá™ üá∫üá∏ üá®üá¶ üá∏üá¨
            </div>
          </article>
        </div>


      </article>

    </div>

  </div>

  <!-- Footer -->


  <footer class="fixed-bottom">
    <div class="container mt-0">
      &copy; Copyright 2023 Sumanth Doddapaneni. All rights reserved.
    </div>
  </footer>



</body>

<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"
  integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg=="
  crossorigin="anonymous"></script>

<!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js"
  integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A=="
  crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"
  integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ=="
  crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js"
  integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw=="
  crossorigin="anonymous"></script>


<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>






<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>